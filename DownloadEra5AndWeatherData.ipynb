{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# !pip install cdsapi xarray matplotlib netCDF4\n",
    "\n",
    "import cdsapi\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CDS API client\n",
    "c = cdsapi.Client()\n",
    "\n",
    "# Define the Alberta bounding box (North, West, South, East)\n",
    "# bbox = [60, -120, 50, -110]  # Alberta, Canada\n",
    "bbox = [90, -170, 24, -50]  # North America\n",
    "pressure_levels = ['250', '500', '850', '1000'] \n",
    "start_year = 1940\n",
    "end_year = 2023\n",
    "\n",
    "# Path to save the downloaded NetCDF file\n",
    "dir = Path(r'D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace')\n",
    "output_file = dir / 'NAmerica_geopotential_1940_2023.nc'\n",
    "\n",
    "# Step 1: Download the data using the CDS API\n",
    "if not output_file.exists():\n",
    "    print(\"Downloading ERA5 data from {0} to {1}...\".format(start_year, end_year))\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-pressure-levels-monthly-means',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'format': 'netcdf',\n",
    "            'variable': ['u_component_of_wind', 'v_component_of_wind', 'geopotential', 'temperature'],  \n",
    "            'pressure_level': pressure_levels,\n",
    "            'year': [str(year) for year in range(start_year, end_year+1)],\n",
    "            'month': [f'{month:02d}' for month in range(1, 13)],\n",
    "            'time': ['12:00'],\n",
    "            'area': bbox,\n",
    "        },\n",
    "        str(output_file)\n",
    "    )\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(f\"File {output_file} already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 2: Load and process the data using xarray\n",
    "if output_file.exists():\n",
    "    # Step 1: Load the data using xarray\n",
    "    data = xr.open_dataset(output_file)\n",
    "\n",
    "    # Step 2: Extract the relevant variables at 250mb\n",
    "    u_250mb = data['u'].sel(pressure_level=250)  # Zonal wind (east-west)\n",
    "    v_250mb = data['v'].sel(pressure_level=250)  # Meridional wind (north-south)\n",
    "    geopotential_250mb = data['z'].sel(pressure_level=250) / 9.81  # Geopotential height (converted to meters)\n",
    "\n",
    "    # Step 3: Calculate the mean over the entire period\n",
    "    u_mean = u_250mb.mean(dim='date')  # Mean of zonal wind over all months\n",
    "    v_mean = v_250mb.mean(dim='date')  # Mean of meridional wind over all months\n",
    "    geo_height_mean = geopotential_250mb.mean(dim='date')  # Mean geopotential height\n",
    "\n",
    "    # Step 4: Calculate the wind speed (magnitude)\n",
    "    wind_speed = np.sqrt(u_mean**2 + v_mean**2)  # Wind speed is the magnitude of (u, v)\n",
    "\n",
    "    # Flatten the plot by setting the contour levels\n",
    "    geo_height_std = geopotential_250mb.std(dim='date')  # Standard deviation of geopotential height\n",
    "    levels_geo = np.linspace((geo_height_mean - 2 * geo_height_std).min(), (geo_height_mean + 2 * geo_height_std).max(), 100)\n",
    "    levels_wind = np.linspace(wind_speed.min(), wind_speed.max(), 40)  # Levels for wind speed\n",
    "\n",
    "    # Step 5: Plot the geopotential height map and wind speed as colored contours\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the geopotential height as a filled contour map\n",
    "    contour_geo = plt.contourf(geo_height_mean.longitude, geo_height_mean.latitude, geo_height_mean, levels=levels_geo, cmap='BuPu')\n",
    "    plt.colorbar(contour_geo, label='Geopotential Height (meters)')\n",
    "\n",
    "    # Plot the wind speed as colored contours\n",
    "    # 'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', \n",
    "    # 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', \n",
    "    # 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', \n",
    "    # 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
    "    # 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', \n",
    "    # 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', \n",
    "    # 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', \n",
    "    # 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', \n",
    "    # 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', \n",
    "    # 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'inferno', 'inferno_r', \n",
    "    # 'jet', 'jet_r', 'magma', 'magma_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', \n",
    "    # 'rainbow', 'rainbow_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', \n",
    "    # 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', \n",
    "    # 'winter', 'winter_r'\n",
    "\n",
    "    contour_wind = plt.contourf(u_mean.longitude, u_mean.latitude, wind_speed, levels=levels_wind, cmap='jet', alpha=0.6)\n",
    "    plt.colorbar(contour_wind, label='Wind Speed (m/s)')\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title('Monthly Averaged Wind Flow Patterns at 250mb (2013-2023)')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"File {output_file} does not exist. Please check the path or download step.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download weather station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "def download_csv_files(url, folder_path):\n",
    "    # Local directory where files will be saved using pathlib\n",
    "    save_dir = Path(folder_path)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    # Get the webpage content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to connect to {url}\")\n",
    "        return\n",
    "\n",
    "    # Parse the webpage content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the CSV file links\n",
    "    for link in soup.find_all('a'):\n",
    "        file_name = link.get('href')\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_url = url + file_name\n",
    "\n",
    "            # Download the CSV file\n",
    "            print(f\"Downloading {file_name}...\")\n",
    "            csv_response = requests.get(file_url)\n",
    "\n",
    "            # Save the CSV file using pathlib\n",
    "            file_path = save_dir / file_name\n",
    "            file_path.write_bytes(csv_response.content)\n",
    "\n",
    "    print(\"All files downloaded!\")\n",
    "\n",
    "# Example usage\n",
    "url = \"https://dd.weather.gc.ca/climate/observations/hourly/csv/AB/\"\n",
    "download_folder = r\"C:\\Users\\Sunbeam\\Downloads\\csv_files\"\n",
    "download_csv_files(url, download_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique stations within the bounding box: 19 stations\n",
      "CORONATION A (ID: 3011880) - 1953 (744), 1954 (744), 1955 (744), 1956 (744), 1957 (744), 1958 (744), 1959 (744), 1960 (744), 1961 (744), 1962 (744), 1963 (744), 1964 (744), 1965 (744), 1966 (744), 1967 (744), 1968 (744), 1969 (744), 1970 (744), 1971 (744), 1972 (744), 1973 (744), 1974 (744), 1975 (744), 1976 (744), 1977 (744), 1978 (739), 1979 (739), 1980 (739), 1981 (739), 1982 (739), 1983 (744), 1984 (744), 1985 (744), 1986 (744), 1987 (744), 1988 (744), 1989 (744), 1990 (744), 1991 (744), 1992 (744), 1993 (744), 1994 (744)\n",
      "CORONATION (AUT) (ID: 3011885) - 1995 (744), 1996 (744), 1997 (744), 1998 (744), 1999 (744), 2000 (744), 2001 (744), 2002 (744), 2003 (744)\n",
      "CORONATION CLIMATE (ID: 3011887) - 2004 (744), 2005 (744), 2006 (744), 2007 (744), 2008 (744), 2009 (744), 2010 (744), 2011 (744), 2012 (744), 2013 (744), 2014 (744), 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (744), 2022 (744), 2023 (744), 2024 (744)\n",
      "DRUMHELLER EAST (ID: 30221LG) - 1995 (744), 1996 (729), 1997 (744), 1998 (744), 1999 (744), 2000 (744), 2001 (743), 2002 (744), 2003 (744), 2004 (744), 2005 (744), 2006 (744), 2007 (744), 2008 (744), 2009 (744), 2010 (744), 2011 (744), 2012 (744), 2013 (744), 2014 (744), 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (744), 2022 (744), 2023 (744), 2024 (744)\n",
      "RED DEER A (ID: 3025480) - 1953 (744), 1954 (744), 1955 (744), 1956 (744), 1957 (744), 1958 (744), 1959 (744), 1960 (744), 1961 (744), 1962 (744), 1963 (744), 1964 (744), 1965 (744), 1966 (744), 1967 (744), 1968 (744), 1969 (744), 1970 (744), 1971 (744), 1972 (744), 1973 (744), 1974 (744), 1975 (744), 1976 (744), 1977 (744), 1978 (744), 1979 (744), 1980 (744), 1981 (744), 1982 (744), 1983 (744), 1984 (744), 1985 (744), 1986 (744), 1987 (744), 1988 (744), 1989 (744), 1990 (744), 1991 (744), 1992 (744), 1993 (744), 1994 (744), 1995 (744), 1996 (744), 1997 (744), 1998 (744), 1999 (744), 2000 (744), 2001 (744), 2002 (744), 2003 (744), 2004 (744), 2005 (744), 2006 (744), 2007 (744), 2008 (744), 2009 (744), 2010 (744), 2011 (744), 2012 (744), 2013 (744), 2014 (744)\n",
      "RED DEER REGIONAL A (ID: 3025481) - 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (1488), 2022 (744), 2023 (744), 2024 (744)\n",
      "RED DEER REGIONAL A (ID: 3025484) - 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (1488), 2022 (744), 2023 (744), 2024 (744)\n",
      "SUNDRE A (ID: 3026KNQ) - 1995 (744), 1996 (729), 1997 (744), 1998 (744), 1999 (744), 2000 (744), 2001 (744), 2002 (744), 2003 (744), 2004 (744), 2005 (744), 2006 (744), 2007 (744), 2008 (744), 2009 (744), 2010 (744), 2011 (744), 2012 (744), 2013 (744), 2014 (744), 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (744), 2022 (744), 2023 (744), 2024 (744)\n",
      "CALGARY INTL A (ID: 3031092) - 2013 (744), 2014 (744), 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (744), 2022 (744), 2023 (744), 2024 (744)\n",
      "CALGARY INT'L A (ID: 3031093) - 1953 (744), 1954 (744), 1955 (744), 1956 (744), 1957 (744), 1958 (744), 1959 (744), 1960 (744), 1961 (744), 1962 (744), 1963 (744), 1964 (744), 1965 (744), 1966 (744), 1967 (744), 1968 (744), 1969 (744), 1970 (744), 1971 (744), 1972 (744), 1973 (744), 1974 (744), 1975 (744), 1976 (744), 1977 (744), 1978 (744), 1979 (744), 1980 (744), 1981 (744), 1982 (744), 1983 (744), 1984 (744), 1985 (744), 1986 (744), 1987 (744), 1988 (744), 1989 (744), 1990 (744), 1991 (744), 1992 (744), 1993 (744), 1994 (744), 1995 (744), 1996 (744), 1997 (744), 1998 (744), 1999 (744), 2000 (744), 2001 (744), 2002 (744), 2003 (744), 2004 (744), 2005 (744), 2006 (744), 2007 (744), 2008 (744), 2009 (744), 2010 (744), 2011 (744), 2012 (744)\n",
      "CALGARY INT'L CS (ID: 3031094) - 2009 (744), 2010 (744), 2011 (744), 2012 (744), 2013 (744), 2014 (744), 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (744), 2022 (744), 2023 (744), 2024 (744)\n",
      "CLARESHOLM (ID: 3031640) - 1995 (744), 1996 (729), 1997 (744), 1998 (744), 1999 (744), 2000 (744), 2001 (744), 2002 (744), 2003 (744), 2004 (744), 2005 (744), 2006 (744), 2007 (744), 2008 (744), 2009 (744), 2010 (744), 2011 (744), 2012 (744), 2013 (744), 2014 (744), 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (744), 2022 (744), 2023 (744), 2024 (744)\n",
      "LETHBRIDGE (ID: 3033875) - 2011 (494), 2012 (744), 2013 (744), 2014 (744), 2015 (742), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (744), 2022 (744), 2023 (744), 2024 (744)\n",
      "LETHBRIDGE A (ID: 3033880) - 1953 (744), 1954 (744), 1955 (744), 1956 (744), 1957 (744), 1958 (744), 1959 (744), 1960 (744), 1961 (744), 1962 (744), 1963 (744), 1964 (744), 1965 (744), 1966 (744), 1967 (744), 1968 (744), 1969 (744), 1970 (744), 1971 (744), 1972 (744), 1973 (744), 1974 (744), 1975 (744), 1976 (744), 1977 (744), 1978 (744), 1979 (744), 1980 (744), 1981 (744), 1982 (744), 1983 (744), 1984 (744), 1985 (744), 1986 (744), 1987 (744), 1988 (744), 1989 (744), 1990 (744), 1991 (744), 1992 (744), 1993 (744), 1994 (744), 1995 (744), 1996 (744), 1997 (744), 1998 (744), 1999 (744), 2000 (744), 2001 (744), 2002 (744), 2003 (744), 2004 (744), 2005 (744), 2006 (744), 2007 (738), 2008 (738), 2009 (738), 2010 (738), 2011 (738), 2012 (738), 2013 (738), 2014 (738), 2015 (738), 2016 (738), 2017 (738), 2018 (738), 2019 (738), 2020 (738), 2021 (738), 2022 (738), 2023 (738), 2024 (738)\n",
      "LETHBRIDGE A (ID: 3033881) - 1953 (744), 1954 (744), 1955 (744), 1956 (744), 1957 (744), 1958 (744), 1959 (744), 1960 (744), 1961 (744), 1962 (744), 1963 (744), 1964 (744), 1965 (744), 1966 (744), 1967 (744), 1968 (744), 1969 (744), 1970 (744), 1971 (744), 1972 (744), 1973 (744), 1974 (744), 1975 (744), 1976 (744), 1977 (744), 1978 (744), 1979 (744), 1980 (744), 1981 (744), 1982 (744), 1983 (744), 1984 (744), 1985 (744), 1986 (744), 1987 (744), 1988 (744), 1989 (744), 1990 (744), 1991 (744), 1992 (744), 1993 (744), 1994 (744), 1995 (744), 1996 (744), 1997 (744), 1998 (744), 1999 (744), 2000 (744), 2001 (744), 2002 (744), 2003 (744), 2004 (744), 2005 (744), 2006 (744), 2007 (738), 2008 (738), 2009 (738), 2010 (738), 2011 (738), 2012 (738), 2013 (738), 2014 (738), 2015 (738), 2016 (738), 2017 (738), 2018 (738), 2019 (738), 2020 (738), 2021 (738), 2022 (738), 2023 (738), 2024 (738)\n",
      "LETHBRIDGE AWOS A (ID: 3033884) - 2007 (744), 2008 (744), 2009 (744), 2010 (744), 2011 (744), 2012 (744)\n",
      "LETHBRIDGE CDA (ID: 3033890) - 1995 (741), 1996 (728), 1997 (744), 1998 (744), 1999 (744), 2000 (744), 2001 (744), 2002 (744), 2003 (744), 2004 (744), 2005 (744), 2006 (744), 2007 (744), 2008 (743), 2009 (744), 2010 (743), 2011 (743), 2012 (743), 2013 (743), 2014 (744), 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (743), 2022 (743), 2023 (743), 2024 (743)\n",
      "STRATHMORE AGDM (ID: 3036205) - 2005 (744), 2006 (744), 2007 (744), 2008 (744), 2009 (744), 2010 (744), 2011 (744), 2012 (683), 2013 (744), 2014 (744), 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (744), 2022 (744), 2023 (744), 2024 (736)\n",
      "MILK RIVER (ID: 3044533) - 1995 (744), 1996 (729), 1997 (744), 1998 (744), 1999 (744), 2000 (744), 2001 (744), 2002 (744), 2003 (744), 2004 (744), 2005 (744), 2006 (744), 2007 (744), 2008 (744), 2009 (744), 2010 (744), 2011 (744), 2012 (744), 2013 (744), 2014 (744), 2015 (744), 2016 (744), 2017 (744), 2018 (744), 2019 (744), 2020 (744), 2021 (744), 2022 (744), 2023 (744), 2024 (744)\n",
      "Filtered shapefile saved at: D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\filtered_stations_BBox_large_1.shp\n",
      "Merged CSV file saved at: D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\merged_weather_data_BBox_large_1.csv\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "def filter_weather_data(csv_dir, months, shp_polygon, weather_param=None):\n",
    "    \"\"\"\n",
    "    Filters weather station data from CSV files for the specified months, creates one point per station,\n",
    "    and populates the point attributes with weather data for each record. Only CSV files with stations \n",
    "    within the bounding box (BBox) are processed.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_dir: Directory containing the CSV files.\n",
    "    - months: List of months to filter data (e.g., [1, 2, 3] for Jan, Feb, Mar).\n",
    "    - shp_polygon: Shapefile containing the bounding box (BBox) for spatial filtering.\n",
    "    - weather_param: List of weather parameters to include (e.g., ['Temp (°C)', 'Wind Dir (10s deg)']).\n",
    "    \"\"\"\n",
    "    \n",
    "    default_columns = ['longitude (x)', 'latitude (y)', 'station name', 'climate id', 'date/time (lst)', 'year', 'month', 'day', 'time (lst)']\n",
    "    weather_param = weather_param or []\n",
    "    original_weather_param = [param.lower() for param in weather_param]\n",
    "    cleaned_weather_param = [re.sub(r'[^a-zA-Z]', '', param.lower())[:10] for param in weather_param]\n",
    "    selected_columns = default_columns + original_weather_param\n",
    "\n",
    "    with arcpy.da.SearchCursor(shp_polygon, [\"SHAPE@\"]) as cursor:\n",
    "        bbox_polygon = next(cursor)[0]\n",
    "\n",
    "    csv_files = list(Path(csv_dir).glob(\"*.csv\"))\n",
    "    station_dict, lat_long_id_dict, all_data = {}, {}, []\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            weather_data = pd.read_csv(csv_file, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "        except UnicodeDecodeError:\n",
    "            weather_data = pd.read_csv(csv_file, encoding='ISO-8859-1', on_bad_lines='skip', engine='python')\n",
    "\n",
    "        weather_data.columns = weather_data.columns.str.strip().str.lower()\n",
    "        if 'station name' not in weather_data.columns: continue\n",
    "\n",
    "        if 'date/time (lst)' in weather_data.columns:\n",
    "            weather_data['date/time (lst)'] = pd.to_datetime(weather_data['date/time (lst)'], errors='coerce')\n",
    "            weather_data = weather_data[weather_data['date/time (lst)'].dt.month.isin(months)]\n",
    "        if weather_data.empty: continue\n",
    "\n",
    "        longitude_col, latitude_col = [col for col in weather_data.columns if 'longitude' in col][0], [col for col in weather_data.columns if 'latitude' in col][0]\n",
    "        weather_data['station_point'] = weather_data.apply(\n",
    "            lambda row: arcpy.PointGeometry(arcpy.Point(row[longitude_col], row[latitude_col]), arcpy.SpatialReference(4326)), axis=1\n",
    "        )\n",
    "        weather_data = weather_data[weather_data['station_point'].apply(lambda pt: pt.within(bbox_polygon))]\n",
    "\n",
    "        if weather_data.empty: continue\n",
    "\n",
    "        for _, row in weather_data.iterrows():\n",
    "            station_name, climate_id = row['station name'], row['climate id']\n",
    "            lat_long_id_key = (row[longitude_col], row[latitude_col], climate_id)\n",
    "\n",
    "            if lat_long_id_key not in lat_long_id_dict:\n",
    "                lat_long_id_dict[lat_long_id_key] = {'station_name': station_name, 'climate_id': climate_id, 'location': row['station_point']}\n",
    "                station_dict.setdefault((station_name, climate_id), {'location': row['station_point'], 'climate_id': climate_id, 'data': pd.DataFrame()})\n",
    "\n",
    "        all_data.append(weather_data[selected_columns])\n",
    "\n",
    "    print(f\"Total unique stations within the bounding box: {len(lat_long_id_dict)} stations\")\n",
    "\n",
    "    # Print year-wise data for each unique station\n",
    "    for (station_name, climate_id), station_info in station_dict.items():\n",
    "        station_data = pd.concat([data for data in all_data if data['station name'].eq(station_name).any()])\n",
    "        station_data['year'] = station_data['date/time (lst)'].dt.year\n",
    "        year_counts = station_data.groupby('year').size()\n",
    "        year_info = ', '.join([f\"{year} ({count})\" for year, count in year_counts.items()])\n",
    "        print(f\"{station_name} (ID: {climate_id}) - {year_info}\")\n",
    "\n",
    "    if lat_long_id_dict:\n",
    "        shapefile_name = Path(shp_polygon).parent / f\"filtered_stations_{Path(shp_polygon).stem}_{'_'.join(map(str, months))}.shp\"\n",
    "        point_features = [info['location'] for info in lat_long_id_dict.values()]\n",
    "        arcpy.CopyFeatures_management(point_features, str(shapefile_name))\n",
    "\n",
    "        arcpy.management.AddFields(str(shapefile_name), [[\"station_n\", \"TEXT\"], [\"climate_id\", \"TEXT\"]] + [[field, \"TEXT\"] for field in cleaned_weather_param])\n",
    "\n",
    "        with arcpy.da.UpdateCursor(str(shapefile_name), [\"station_n\", \"climate_id\"] + cleaned_weather_param) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                station_info = list(lat_long_id_dict.values())[i]\n",
    "                row[0], row[1] = station_info['station_name'], station_info['climate_id']\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "        print(f\"Filtered shapefile saved at: {shapefile_name}\")\n",
    "\n",
    "    merged_csv_path = Path(shp_polygon).parent / f\"merged_weather_data_{Path(shp_polygon).stem}_{'_'.join(map(str, months))}.csv\"\n",
    "    merged_data = pd.concat(all_data, ignore_index=True)\n",
    "    merged_data.rename(columns={original: cleaned for original, cleaned in zip(original_weather_param, cleaned_weather_param)}, inplace=True)\n",
    "    merged_data.to_csv(merged_csv_path, index=False)\n",
    "    print(f\"Merged CSV file saved at: {merged_csv_path}\")\n",
    "\n",
    "csv_dir = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Station_Data_csv\"\n",
    "months = [1]\n",
    "# shp_polygon = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\BBox_small.shp\"\n",
    "shp_polygon = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\BBox_large.shp\"\n",
    "weather_param = ['Temp (°C)', 'Wind Dir (10s deg)']\n",
    "\n",
    "filter_weather_data(csv_dir, months, shp_polygon, weather_param)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
