{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download ECCC weather station data from https://dd.weather.gc.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "def download_eccc_csv_files(url, folder_path):\n",
    "    # Local directory where files will be saved using pathlib\n",
    "    save_dir = Path(folder_path)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    # Get the webpage content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to connect to {url}\")\n",
    "        return\n",
    "\n",
    "    # Parse the webpage content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the CSV file links\n",
    "    for link in soup.find_all('a'):\n",
    "        file_name = link.get('href')\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_url = url + file_name\n",
    "\n",
    "            # Download the CSV file\n",
    "            print(f\"Downloading {file_name}...\")\n",
    "            csv_response = requests.get(file_url)\n",
    "\n",
    "            # Save the CSV file using pathlib\n",
    "            file_path = save_dir / file_name\n",
    "            file_path.write_bytes(csv_response.content)\n",
    "\n",
    "    print(\"All files downloaded!\")\n",
    "\n",
    "# Example usage\n",
    "url = \"https://dd.weather.gc.ca/climate/observations/hourly/csv/AB/\"\n",
    "download_folder = r\"C:\\Users\\Sunbeam\\Downloads\\csv_files\"\n",
    "download_eccc_csv_files(url, download_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter and analyze ECCC data suing bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "def filter_eccc_station_data(csv_dir, months, shp_bbox, weather_param=None):\n",
    "    \"\"\"\n",
    "    Filters weather data from CSV files in a directory based on specified months and spatial polygon, \n",
    "    merges the data, saves it as a combined CSV, and generates a summary per station.\n",
    "\n",
    "    Parameters:\n",
    "    csv_dir (str): Directory containing CSV files with weather data.\n",
    "    months (list): List of months (as integers) to filter the data.\n",
    "    shp_polygon (str): Path to the shapefile containing the polygon for spatial filtering.\n",
    "    weather_param (list, optional): List of weather parameters to include in the filtered data.\n",
    "    \"\"\"\n",
    "    \n",
    "    default_columns = ['longitude (x)', 'latitude (y)', 'station name', 'climate id', 'date/time (lst)', 'year', 'month', 'day', 'time (lst)']\n",
    "    weather_param = weather_param or []\n",
    "    original_weather_param = [param.lower() for param in weather_param]\n",
    "    cleaned_weather_param = [re.sub(r'[^a-zA-Z]', '', param.lower())[:10] for param in weather_param]\n",
    "    selected_columns = default_columns + original_weather_param\n",
    "\n",
    "    with arcpy.da.SearchCursor(shp_bbox, [\"SHAPE@\"]) as cursor:\n",
    "        bbox_polygon = next(cursor)[0]\n",
    "\n",
    "    csv_files = list(Path(csv_dir).glob(\"*.csv\"))\n",
    "    station_dict, lat_long_id_dict, all_data = {}, {}, []\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            weather_data = pd.read_csv(csv_file, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "        except UnicodeDecodeError:\n",
    "            weather_data = pd.read_csv(csv_file, encoding='ISO-8859-1', on_bad_lines='skip', engine='python')\n",
    "\n",
    "        weather_data.columns = weather_data.columns.str.strip().str.lower()\n",
    "        if 'station name' not in weather_data.columns:\n",
    "            continue\n",
    "\n",
    "        if 'date/time (lst)' in weather_data.columns:\n",
    "            weather_data['date/time (lst)'] = pd.to_datetime(weather_data['date/time (lst)'], errors='coerce')\n",
    "            weather_data = weather_data[weather_data['date/time (lst)'].dt.month.isin(months)]\n",
    "        if weather_data.empty:\n",
    "            continue\n",
    "\n",
    "        longitude_col = [col for col in weather_data.columns if 'longitude' in col][0]\n",
    "        latitude_col = [col for col in weather_data.columns if 'latitude' in col][0]\n",
    "        weather_data['station_point'] = weather_data.apply(\n",
    "            lambda row: arcpy.PointGeometry(arcpy.Point(row[longitude_col], row[latitude_col]), arcpy.SpatialReference(4326)), axis=1\n",
    "        )\n",
    "        weather_data = weather_data[weather_data['station_point'].apply(lambda pt: pt.within(bbox_polygon))]\n",
    "\n",
    "        if weather_data.empty:\n",
    "            continue\n",
    "\n",
    "        for _, row in weather_data.iterrows():\n",
    "            lat_long_id_key = (row[longitude_col], row[latitude_col], row['climate id'])\n",
    "            if lat_long_id_key not in lat_long_id_dict:\n",
    "                lat_long_id_dict[lat_long_id_key] = {\n",
    "                    'station_name': row['station name'],\n",
    "                    'climate_id': row['climate id'],\n",
    "                    'location': row['station_point']\n",
    "                }\n",
    "                station_dict.setdefault((row['station name'], row['climate id']), {'location': row['station_point'], 'data': pd.DataFrame()})\n",
    "\n",
    "        all_data.append(weather_data[selected_columns])\n",
    "\n",
    "    for (station_name, climate_id), station_info in station_dict.items():\n",
    "        station_data = pd.concat([data for data in all_data if data['station name'].eq(station_name).any()])\n",
    "        station_data['year'] = station_data['date/time (lst)'].dt.year\n",
    "        year_counts = station_data.groupby('year').size()\n",
    "        year_info = ', '.join([f\"{year} ({count})\" for year, count in year_counts.items()])\n",
    "        print(f\"{station_name} (ID: {climate_id}) - {year_info}\")\n",
    "    \n",
    "    if lat_long_id_dict:\n",
    "        shapefile_name = Path(shp_bbox).parent / f\"filtered_stations_{Path(shp_bbox).stem}_{'_'.join(map(str, months))}.shp\"\n",
    "        point_features = [info['location'] for info in lat_long_id_dict.values()]\n",
    "        arcpy.CopyFeatures_management(point_features, str(shapefile_name))\n",
    "        arcpy.management.AddFields(str(shapefile_name), [[\"station_n\", \"TEXT\"], [\"climate_id\", \"TEXT\"]] + [[field, \"TEXT\"] for field in cleaned_weather_param])\n",
    "\n",
    "        with arcpy.da.UpdateCursor(str(shapefile_name), [\"station_n\", \"climate_id\"] + cleaned_weather_param) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                key = list(lat_long_id_dict.keys())[i]\n",
    "                row[0], row[1] = lat_long_id_dict[key]['station_name'], lat_long_id_dict[key]['climate_id']\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    # Merge all the data and save as a CSV\n",
    "    merged_csv_path = Path(shp_bbox).parent / f\"ECCC_station_data_{Path(shp_bbox).stem}_{'_'.join(map(str, months))}.csv\"\n",
    "    merged_data = pd.concat(all_data, ignore_index=True)\n",
    "    merged_data.rename(columns={original: cleaned for original, cleaned in zip(original_weather_param, cleaned_weather_param)}, inplace=True)\n",
    "    merged_data.to_csv(merged_csv_path, index=False)\n",
    "    print(f\"Merged CSV file saved at: {merged_csv_path}\")\n",
    "\n",
    "    summary = []\n",
    "    for station_name, station_data in merged_data.groupby('station name'):\n",
    "        null_counts = station_data[cleaned_weather_param].isnull().sum().to_dict()\n",
    "\n",
    "        summary.append({\n",
    "            'Station name': station_name,\n",
    "            'First Year': station_data['date/time (lst)'].dt.year.min(),\n",
    "            'Total Records': len(station_data),\n",
    "            **{f\"Null Values - {param}\": null_counts.get(param, 0) for param in cleaned_weather_param}\n",
    "        })\n",
    "\n",
    "    # Save the summary as a CSV\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    summary_output_path = Path(shp_bbox).parent / f\"ECCC_station_summary_{Path(shp_bbox).stem}_{'_'.join(map(str, months))}.csv\"\n",
    "    summary_df.to_csv(summary_output_path, index=False)\n",
    "    print(f\"Station summary saved at: {summary_output_path}\")\n",
    "    \n",
    "# Example usage\n",
    "csv_dir = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ECCC_Data\"\n",
    "months = [12, 1, 2]\n",
    "# shp_bbox = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\BBox_small.shp\"\n",
    "shp_bbox = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\BBox_large.shp\"\n",
    "weather_param = ['Temp (°C)', 'Wind Dir (10s deg)']\n",
    "\n",
    "filter_eccc_station_data(csv_dir, months, shp_bbox, weather_param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ECCC binned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in wind_dir_210_279 group: ['210-225', '225-240', '240-255', '255-270']\n",
      "Processed data has been saved to D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\Windir_1950_2024_Dec\\wind_direction_15deg_yearly_data_stream-oper_BBox_large_masked_5-yearly_pivoted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def process_eccc_station_data(input_csv, aggregation='yearly'):\n",
    "    \"\"\"\n",
    "    Processes weather station data from a CSV file, handling both individual measurements and binned counts.\n",
    "    Counts wind directions within specific groups (210-279 and 280-209) and outputs a pivoted CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv: Path to the input CSV file containing weather station data.\n",
    "    - aggregation: Aggregation level, either 'yearly' or '5-yearly'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv, low_memory=False)\n",
    "    # df = pd.read_excel(input_csv, sheet_name='Sheet3')\n",
    "\n",
    "    # Detect if CSV format is binned (wind direction bins) or individual measurements\n",
    "    is_binned_format = any(re.match(r'^\\d+-\\d+$', col) for col in df.columns)\n",
    "\n",
    "    if is_binned_format:        \n",
    "        dir_210_279_bins = [col for col in df.columns if re.match(r'^(21[0-9]|2[0-7][0-9])-2[0-7][0-9]$', col)]\n",
    "        dir_other_bins = [col for col in df.columns if re.match(r'^\\d+-\\d+$', col) and col not in dir_210_279_bins]\n",
    "\n",
    "        print(\"Columns in wind_dir_210_279 group:\", dir_210_279_bins)\n",
    "\n",
    "        df['wind_dir_210_279'] = df[dir_210_279_bins].sum(axis=1)\n",
    "        df['wind_dir_280_209'] = df[dir_other_bins].sum(axis=1)\n",
    "    else:\n",
    "        df = df.dropna(subset=['winddirsde'])\n",
    "        df['actual_wind_dir'] = df['winddirsde'] * 10\n",
    "        df['wind_dir_210_279'] = df['actual_wind_dir'].between(210, 279).astype(int)\n",
    "        df['wind_dir_280_209'] = (~df['actual_wind_dir'].between(210, 279)).astype(int)\n",
    "\n",
    "    if aggregation == '5-yearly':\n",
    "        df['aggregation_period'] = (df['year'] // 5 * 5).astype(str) + '-' + (df['year'] // 5 * 5 + 4).astype(str)\n",
    "        pivot_table = df.pivot_table(\n",
    "            values=['wind_dir_210_279', 'wind_dir_280_209'],\n",
    "            index=['aggregation_period', 'month'],\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "    elif aggregation == 'yearly':\n",
    "        df['aggregation_period'] = df['year'].astype(str)\n",
    "        pivot_table = df.pivot_table(\n",
    "            values=['wind_dir_210_279', 'wind_dir_280_209'],\n",
    "            index=['aggregation_period', 'month'],\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid aggregation level. Please specify either 'yearly' or '5-yearly'.\")\n",
    "\n",
    "    # Save the results to a new CSV file\n",
    "    output_csv = Path(input_csv).with_name(f\"{Path(input_csv).stem}_{aggregation}_pivoted.csv\")\n",
    "    pivot_table.to_csv(output_csv, index=False)\n",
    "    print(f\"Processed data has been saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "# input_csv_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\Windir_1950_2024_Dec\\wind_direction_15deg_yearly_data_stream-oper_BBox_small_masked.csv\"\n",
    "input_csv_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\Windir_1950_2024_Dec\\wind_direction_15deg_yearly_data_stream-oper_BBox_large_masked.csv\"\n",
    "process_eccc_station_data(input_csv_path, aggregation='5-yearly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process ECCC binned wind direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def process_eccc_binned_data(input_csv_file, bin_size=15, month_count=True, aggregate=\"yearly\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate and save wind direction hour counts to a CSV file, with optional grouping by month and aggregation period.\n",
    "\n",
    "    Parameters:\n",
    "    input_csv_file (str): Path to the input CSV file containing wind direction data.\n",
    "    bin_size (int): Size of the bins for wind direction intervals in degrees. Default is 15.\n",
    "    month_count (bool): If True, groups data by month; if False, aggregates all months of each year with a placeholder month.\n",
    "    aggregate (str): Aggregation period - 'yearly' (default) or '5-yearly'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_csv_file, low_memory=False)\n",
    "    if not all(col in df.columns for col in ['station name', 'year', 'winddirsde']):\n",
    "        raise ValueError(\"Input CSV must contain 'station name', 'year', and 'winddirsde' columns.\")\n",
    "    df.dropna(subset=['station name', 'year', 'winddirsde'], inplace=True)\n",
    "    df['winddirsde'] *= 10  # Ensure wind direction is on 0-360 degree scale\n",
    "\n",
    "    # Define bins and labels for wind direction intervals\n",
    "    bins = np.arange(0, 360 + bin_size, bin_size)\n",
    "    labels = [f\"{int(b)}-{int(b + bin_size)}\" for b in bins[:-1]]\n",
    "    df['wind_dir_bin'] = pd.cut(df['winddirsde'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Handle aggregation periods\n",
    "    if aggregate == \"5-yearly\":\n",
    "        df['year'] = (df['year'] // 5) * 5\n",
    "        df['year'] = df['year'].astype(str) + \"-\" + (df['year'].astype(int) + 4).astype(str)\n",
    "\n",
    "    # Handle month_count parameter\n",
    "    if not month_count:\n",
    "        df['month'] = \"99\"  # Placeholder for aggregated months\n",
    "\n",
    "    # Group data and count occurrences within each bin\n",
    "    group_columns = ['station name', 'year'] + (['month'] if month_count else []) + ['wind_dir_bin']\n",
    "    wind_direction_counts = df.groupby(group_columns).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    # Save output to CSV\n",
    "    output_file = Path(input_csv_file).with_name(\n",
    "        f\"ECCC_wind_dir_count_{bin_size}deg_{aggregate}_{'monthly' if month_count else 'yearly'}_{Path(input_csv_file).stem}.csv\"\n",
    "    )\n",
    "    wind_direction_counts.to_csv(output_file, index=False)\n",
    "    print(f\"Wind direction counts saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_file = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\ECCC_station_data_BBox_small_12_1_2_filled.csv\"\n",
    "process_eccc_binned_data(input_csv_file, month_count=True, aggregate=\"5-yearly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process wind rose for each station from binned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_windrose_from_csv(csv_file, charts_per_row=10, max_rmax=None):\n",
    "    \"\"\"\n",
    "    Plots wind rose charts from a CSV file containing wind direction frequency data.\n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file containing wind data. The CSV must contain 'station name', 'year', and 'month' columns, along with columns \n",
    "                    for wind direction bins.\n",
    "    charts_per_row (int, optional): Number of charts to display per row in the output figure. Default is 10.\n",
    "    max_rmax (float, optional): Maximum radius for the wind rose plots. If None, it will be set to 1.1 times the maximum frequency value \n",
    "                                in the data. Default is None.    \n",
    "    \"\"\"\n",
    "\n",
    "    wind_data = pd.read_csv(csv_file)\n",
    "    required_columns = {'station name', 'year', 'month'}\n",
    "    if not required_columns.issubset(wind_data.columns):\n",
    "        raise ValueError(\"Input CSV must contain 'station name', 'year', and 'month' columns.\")\n",
    "\n",
    "    direction_bin_columns = wind_data.columns.difference(['station name', 'year', 'month'])\n",
    "    frequency_by_bin = wind_data.set_index(['station name', 'year', 'month'])[direction_bin_columns]\n",
    "    angles = [np.radians((float(b.split('-')[0]) + float(b.split('-')[1])) / 2) for b in direction_bin_columns]\n",
    "    manual_rmax = frequency_by_bin.values.max() * 1.1 if max_rmax is None else max_rmax\n",
    "\n",
    "    for station, station_data in frequency_by_bin.groupby(level=0):\n",
    "        num_years = len(station_data.index.get_level_values('year').unique())\n",
    "        num_rows = (num_years + charts_per_row - 1) // charts_per_row\n",
    "        fig, axes = plt.subplots(num_rows, charts_per_row, subplot_kw=dict(projection='polar'),\n",
    "                                 figsize=(5 * charts_per_row, 5 * num_rows))\n",
    "        axes = axes.flatten()\n",
    "        fig.suptitle(f\"Wind Roses for {station}\", fontsize=16, fontweight='bold', y=1.05, ha='center')\n",
    "\n",
    "        for axis_index, ((year, month), data) in enumerate(station_data.groupby(level=[1, 2])):\n",
    "            if axis_index >= len(axes):\n",
    "                break\n",
    "            ax = axes[axis_index]\n",
    "            ax.fill_between(np.linspace(np.radians(210), np.radians(279), 100), 0, manual_rmax, color='peachpuff')\n",
    "            ax.bar(angles, data.iloc[0], width=np.radians(15), edgecolor='black', color='teal')\n",
    "            ax.set_rmax(manual_rmax)\n",
    "            ax.set_theta_zero_location('N')\n",
    "            ax.set_theta_direction(-1)\n",
    "            ax.set_rlabel_position(202.5)\n",
    "            ax.set_title(f\"{year}\" if '-' in str(year) else f\"{year}-{month:02}\", fontsize=12, fontweight='bold')\n",
    "            ax.set_rticks(np.linspace(0, manual_rmax, num=5))\n",
    "            ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}'))\n",
    "\n",
    "        for j in range(axis_index + 1, len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "csv_file_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\ECCC_wind_dir_count_15deg_5-yearly_ECCC_station_data_BBox_small_12_1_2_filled.csv\"\n",
    "plot_windrose_from_csv(csv_file_path, max_rmax=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process ACIS weather station data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate and summerize the ACIS data, can also be used for ECCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def summerize_acis_data(acis_data_path, coords_data_path):\n",
    "    acis_data_path = Path(acis_data_path)\n",
    "    output_dir = acis_data_path.parent\n",
    "    \n",
    "    # Load data\n",
    "    acis_data = pd.read_csv(acis_data_path, low_memory=False)\n",
    "    coords_data = pd.read_excel(coords_data_path)\n",
    "    \n",
    "    # Process data\n",
    "    wd_at_data = acis_data[acis_data['SENSOR_CD'].isin(['WD', 'AT'])].copy()\n",
    "    wd_at_data['TIMESTAMP'] = pd.to_datetime(wd_at_data['TIMESTAMP'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
    "    wd_at_data['READING'] = pd.to_numeric(wd_at_data['READING'], errors='coerce')\n",
    "    \n",
    "    wd_at_pivot = wd_at_data.pivot_table(index=['NAME', 'TIMESTAMP'], \n",
    "                                         columns='SENSOR_CD', \n",
    "                                         values='READING', \n",
    "                                         aggfunc='first').reset_index()\n",
    "    wd_at_pivot.columns.name = None\n",
    "    \n",
    "    merged_data = pd.merge(wd_at_pivot, coords_data, left_on='NAME', right_on='Station names', how='left')[\n",
    "        ['NAME', 'TIMESTAMP', 'WD', 'AT', 'X', 'Y']\n",
    "    ]\n",
    "    merged_data.columns = ['station name', 'datetime', 'winddir', 'atmostemp', 'longitude', 'latitude']\n",
    "    \n",
    "    # Generate merged CSV\n",
    "    output_path = output_dir / \"ACIS_station_data.csv\"\n",
    "    merged_data.to_csv(output_path, index=False)\n",
    "    print(f\"Merged data saved to '{output_path}'.\")\n",
    "\n",
    "    # Generate summary for each station\n",
    "    summary = []\n",
    "    for station, group in merged_data.groupby('station name'):\n",
    "        first_year = group['datetime'].dt.year.min()\n",
    "        total_records = len(group)\n",
    "        null_counts = group[['winddir', 'atmostemp']].isnull().sum().to_dict()  # Count nulls for each parameter\n",
    "        \n",
    "        summary.append({\n",
    "            'Station name': station,\n",
    "            'First Year': first_year,\n",
    "            'Total Records': total_records,\n",
    "            'Null Values - winddir': null_counts.get('winddir', 0),\n",
    "            'Null Values - atmostemp': null_counts.get('atmostemp', 0)\n",
    "        })\n",
    "    \n",
    "    # Convert summary to DataFrame\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    \n",
    "    # Generate summary CSV\n",
    "    summary_output_path = output_dir / \"ACIS_station_summary.csv\"\n",
    "    summary_df.to_csv(summary_output_path, index=False)\n",
    "    print(f\"Station summary saved to '{summary_output_path}'.\")\n",
    "\n",
    "# Example usage\n",
    "acis_data_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ACIS_Data\\select stn hrly data dec-feb at wd.csv\"\n",
    "coords_data_path =  r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ACIS_Data\\ACIS_Station_names.xlsx\"\n",
    "summerize_acis_data(acis_data_path, coords_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter ACIS data using bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "def filter_acis_station_data(csv_path, months, shp_polygon, weather_param=None):\n",
    "    \"\"\"\n",
    "    Filters ACIS weather station data based on specified months and a spatial polygon,\n",
    "    and saves the filtered data as a CSV.\n",
    "\n",
    "    Parameters:\n",
    "    csv_path (str): Path to the ACIS merged CSV file.\n",
    "    months (list): List of months (as integers) to filter the data.\n",
    "    shp_polygon (str): Path to the shapefile containing the polygon for spatial filtering.\n",
    "    weather_param (list, optional): List of weather parameters to include in the filtered data. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    default_columns = ['longitude', 'latitude', 'station name', 'datetime']\n",
    "    weather_param = weather_param or ['winddir', 'atmostemp']\n",
    "    \n",
    "    # Load data\n",
    "    acis_data = pd.read_csv(csv_path, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "    acis_data.columns = acis_data.columns.str.strip().str.lower()\n",
    "\n",
    "    # Check that all columns are in the DataFrame\n",
    "    selected_columns = default_columns + [col for col in weather_param if col in acis_data.columns]\n",
    "\n",
    "    # Filter by specified months\n",
    "    acis_data['datetime'] = pd.to_datetime(acis_data['datetime'], errors='coerce')\n",
    "    acis_data = acis_data[acis_data['datetime'].dt.month.isin(months)]\n",
    "    if acis_data.empty:\n",
    "        print(\"No data available for the specified months.\")\n",
    "        return\n",
    "\n",
    "    # Spatial filtering\n",
    "    with arcpy.da.SearchCursor(shp_polygon, [\"SHAPE@\"]) as cursor:\n",
    "        bbox_polygon = next(cursor)[0]\n",
    "\n",
    "    acis_data['station_point'] = acis_data.apply(\n",
    "        lambda row: arcpy.PointGeometry(arcpy.Point(row['longitude'], row['latitude']), arcpy.SpatialReference(4326)), axis=1\n",
    "    )\n",
    "    acis_data = acis_data[acis_data['station_point'].apply(lambda pt: pt.within(bbox_polygon))]\n",
    "\n",
    "    if acis_data.empty:\n",
    "        print(\"No data found within the specified polygon.\")\n",
    "        return\n",
    "\n",
    "    # Count unique stations within the bounding box\n",
    "    unique_stations_count = acis_data['station name'].nunique()\n",
    "    print(f\"Number of stations within the bounding box: {unique_stations_count}\")\n",
    "\n",
    "    # Select only the columns that exist in the data\n",
    "    acis_data = acis_data[selected_columns]\n",
    "    \n",
    "    # Output filtered data as CSV\n",
    "    output_path = Path(shp_polygon).parent / f\"acis_filtered_acis_data_{Path(shp_polygon).stem}_{'_'.join(map(str, months))}.csv\"\n",
    "    acis_data.to_csv(output_path, index=False)\n",
    "    print(f\"Filtered ACIS data saved at: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "csv_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ACIS_Data\\merged_station_data.csv\"\n",
    "months = [1]\n",
    "shp_bbox = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\BBox_large.shp\"\n",
    "weather_param = ['winddir', 'atmostemp']\n",
    "\n",
    "filter_acis_station_data(csv_path, months, shp_bbox, weather_param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process ACIS binned wind direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def process_acis_binned_data(input_csv_file, bin_size=15, cutoff_day=None):\n",
    "    \"\"\"\n",
    "    Processes ACIS wind data from a CSV to calculate the number of hours the wind falls within specific direction bins \n",
    "    for each station and month, preserving hourly counts.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv_file: Path to the input CSV file.\n",
    "    - bin_size: Wind direction bin size in degrees (default is 15°).\n",
    "    - cutoff_day: Limits data to days 1 through cutoff_day per month (default is None for full month).\n",
    "    \"\"\"\n",
    "    # Load CSV data\n",
    "    df = pd.read_csv(input_csv_file)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    df.dropna(subset=['datetime', 'winddir', 'station name'], inplace=True)\n",
    "    \n",
    "    # Apply day cutoff if specified\n",
    "    if cutoff_day:\n",
    "        df = df[df['datetime'].dt.day <= cutoff_day]\n",
    "    \n",
    "    # Extract year, month, and ensure wind direction is in degrees\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    \n",
    "    # Bin wind direction data into specified intervals\n",
    "    bins = np.arange(0, 360 + bin_size, bin_size)\n",
    "    df['wind_dir_bin'] = pd.cut(df['winddir'], bins=bins, \n",
    "                                labels=[f\"{int(b)}-{int(b + bin_size)}\" for b in bins[:-1]], right=False)\n",
    "\n",
    "    # Count occurrences within each bin by station, year, and month\n",
    "    wind_direction_counts = df.groupby(['station name', 'year', 'month', 'wind_dir_bin']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Save output to CSV\n",
    "    output_csv_file = Path(input_csv_file).with_name(f\"acis_wind_direction_{bin_size}deg_{Path(input_csv_file).stem}.csv\")\n",
    "    wind_direction_counts.to_csv(output_csv_file)\n",
    "    print(f\"Wind direction hour counts saved to {output_csv_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_file = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\filtered_acis_data_BBox_large_1.csv\"\n",
    "process_acis_binned_data(input_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process ACIS zonal wind direction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def process_acis_binned_data(input_csv, aggregation='yearly'):\n",
    "    \"\"\"\n",
    "    Processes ACIS weather station data with pre-binned wind directions, counting hours within specific direction groups\n",
    "    (210-279 and 280-209) and outputs a pivoted CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv: Path to the input ACIS CSV file containing pre-binned wind direction data.\n",
    "    - aggregation: Aggregation level, either 'yearly' or '5-yearly'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv, low_memory=False)\n",
    "\n",
    "    # Identify columns corresponding to each wind direction bin\n",
    "    dir_210_279_bins = [col for col in df.columns if re.match(r'^(21[0-9]|2[0-7][0-9])-2[0-7][0-9]$', col)]\n",
    "    dir_other_bins = [col for col in df.columns if re.match(r'^\\d+-\\d+$', col) and col not in dir_210_279_bins]\n",
    "\n",
    "    # Sum counts for the two direction groups\n",
    "    df['wind_dir_210_279'] = df[dir_210_279_bins].sum(axis=1)\n",
    "    df['wind_dir_280_209'] = df[dir_other_bins].sum(axis=1)\n",
    "\n",
    "    # Determine aggregation period\n",
    "    if aggregation == '5-yearly':\n",
    "        df['aggregation_period'] = (df['year'] // 5 * 5).astype(str) + '-' + (df['year'] // 5 * 5 + 4).astype(str)\n",
    "        pivot_table = df.pivot_table(\n",
    "            values=['wind_dir_210_279', 'wind_dir_280_209'],\n",
    "            index=['station name', 'aggregation_period', 'month'],\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "    elif aggregation == 'yearly':\n",
    "        df['aggregation_period'] = df['year'].astype(str)\n",
    "        pivot_table = df.pivot_table(\n",
    "            values=['wind_dir_210_279', 'wind_dir_280_209'],\n",
    "            index=['station name', 'aggregation_period', 'month'],\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid aggregation level. Please specify either 'yearly' or '5-yearly'.\")\n",
    "\n",
    "    # Save the results to a new CSV file\n",
    "    output_csv = Path(input_csv).with_name(f\"{Path(input_csv).stem}_{aggregation}_pivoted.csv\")\n",
    "    pivot_table.to_csv(output_csv, index=False)\n",
    "    print(f\"Processed data has been saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\acis_wind_direction_15deg_filtered_acis_data_BBox_large_1.csv\"\n",
    "process_acis_binned_data(input_csv_path, aggregation='yearly')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
