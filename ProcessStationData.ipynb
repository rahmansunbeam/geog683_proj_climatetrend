{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download ECCC weather station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "def download_csv_files(url, folder_path):\n",
    "    # Local directory where files will be saved using pathlib\n",
    "    save_dir = Path(folder_path)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    # Get the webpage content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to connect to {url}\")\n",
    "        return\n",
    "\n",
    "    # Parse the webpage content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the CSV file links\n",
    "    for link in soup.find_all('a'):\n",
    "        file_name = link.get('href')\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_url = url + file_name\n",
    "\n",
    "            # Download the CSV file\n",
    "            print(f\"Downloading {file_name}...\")\n",
    "            csv_response = requests.get(file_url)\n",
    "\n",
    "            # Save the CSV file using pathlib\n",
    "            file_path = save_dir / file_name\n",
    "            file_path.write_bytes(csv_response.content)\n",
    "\n",
    "    print(\"All files downloaded!\")\n",
    "\n",
    "# Example usage\n",
    "url = \"https://dd.weather.gc.ca/climate/observations/hourly/csv/AB/\"\n",
    "download_folder = r\"C:\\Users\\Sunbeam\\Downloads\\csv_files\"\n",
    "download_csv_files(url, download_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter and analyze station data and create a merged csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "def filter_weather_data(csv_dir, months, shp_polygon, weather_param=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Filters weather data from CSV files based on specified months and spatial polygon, and saves the filtered data as a combined CSV.\n",
    "\n",
    "    Parameters:\n",
    "    csv_dir (str): Directory containing the CSV files with weather data.\n",
    "    months (list): List of months (as integers) to filter the data.\n",
    "    shp_polygon (str): Path to the shapefile containing the polygon for spatial filtering.\n",
    "    weather_param (list, optional): List of weather parameters to include in the filtered data. Defaults to None.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    default_columns = ['longitude (x)', 'latitude (y)', 'station name', 'climate id', 'date/time (lst)', 'year', 'month', 'day', 'time (lst)']\n",
    "    weather_param = weather_param or []\n",
    "    original_weather_param = [param.lower() for param in weather_param]\n",
    "    cleaned_weather_param = [re.sub(r'[^a-zA-Z]', '', param.lower())[:10] for param in weather_param]\n",
    "    selected_columns = default_columns + original_weather_param\n",
    "\n",
    "    with arcpy.da.SearchCursor(shp_polygon, [\"SHAPE@\"]) as cursor:\n",
    "        bbox_polygon = next(cursor)[0]\n",
    "\n",
    "    csv_files = list(Path(csv_dir).glob(\"*.csv\"))\n",
    "    station_dict, lat_long_id_dict, all_data = {}, {}, []\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            weather_data = pd.read_csv(csv_file, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "        except UnicodeDecodeError:\n",
    "            weather_data = pd.read_csv(csv_file, encoding='ISO-8859-1', on_bad_lines='skip', engine='python')\n",
    "\n",
    "        weather_data.columns = weather_data.columns.str.strip().str.lower()\n",
    "        if 'station name' not in weather_data.columns:\n",
    "            continue\n",
    "\n",
    "        if 'date/time (lst)' in weather_data.columns:\n",
    "            weather_data['date/time (lst)'] = pd.to_datetime(weather_data['date/time (lst)'], errors='coerce')\n",
    "            weather_data = weather_data[weather_data['date/time (lst)'].dt.month.isin(months)]\n",
    "        if weather_data.empty:\n",
    "            continue\n",
    "\n",
    "        longitude_col, latitude_col = [col for col in weather_data.columns if 'longitude' in col][0], [col for col in weather_data.columns if 'latitude' in col][0]\n",
    "        weather_data['station_point'] = weather_data.apply(\n",
    "            lambda row: arcpy.PointGeometry(arcpy.Point(row[longitude_col], row[latitude_col]), arcpy.SpatialReference(4326)), axis=1\n",
    "        )\n",
    "        weather_data = weather_data[weather_data['station_point'].apply(lambda pt: pt.within(bbox_polygon))]\n",
    "\n",
    "        if weather_data.empty:\n",
    "            continue\n",
    "\n",
    "        for _, row in weather_data.iterrows():\n",
    "            lat_long_id_key = (row[longitude_col], row[latitude_col], row['climate id'])\n",
    "            if lat_long_id_key not in lat_long_id_dict:\n",
    "                lat_long_id_dict[lat_long_id_key] = {\n",
    "                    'station_name': row['station name'],\n",
    "                    'climate_id': row['climate id'],\n",
    "                    'location': row['station_point']\n",
    "                }\n",
    "                station_dict.setdefault((row['station name'], row['climate id']), {'location': row['station_point'], 'data': pd.DataFrame()})\n",
    "\n",
    "        all_data.append(weather_data[selected_columns])\n",
    "\n",
    "    print(f\"Total unique stations processed from all CSVs: {len(station_dict)}\")\n",
    "\n",
    "    for (station_name, climate_id), station_info in station_dict.items():\n",
    "        station_data = pd.concat([data for data in all_data if data['station name'].eq(station_name).any()])\n",
    "        station_data['year'] = station_data['date/time (lst)'].dt.year\n",
    "        year_counts = station_data.groupby('year').size()\n",
    "        year_info = ', '.join([f\"{year} ({count})\" for year, count in year_counts.items()])\n",
    "        print(f\"{station_name} (ID: {climate_id}) - {year_info}\")\n",
    "\n",
    "    if lat_long_id_dict:\n",
    "        shapefile_name = Path(shp_polygon).parent / f\"filtered_stations_{Path(shp_polygon).stem}_{'_'.join(map(str, months))}.shp\"\n",
    "        point_features = [info['location'] for info in lat_long_id_dict.values()]\n",
    "        arcpy.CopyFeatures_management(point_features, str(shapefile_name))\n",
    "\n",
    "        arcpy.management.AddFields(str(shapefile_name), [[\"station_n\", \"TEXT\"], [\"climate_id\", \"TEXT\"]] + [[field, \"TEXT\"] for field in cleaned_weather_param])\n",
    "\n",
    "        with arcpy.da.UpdateCursor(str(shapefile_name), [\"station_n\", \"climate_id\"] + cleaned_weather_param) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                key = list(lat_long_id_dict.keys())[i]\n",
    "                row[0], row[1] = lat_long_id_dict[key]['station_name'], lat_long_id_dict[key]['climate_id']\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "        print(f\"Filtered shapefile saved at: {shapefile_name}\")\n",
    "\n",
    "    merged_csv_path = Path(shp_polygon).parent / f\"merged_weather_data_{Path(shp_polygon).stem}_{'_'.join(map(str, months))}.csv\"\n",
    "    merged_data = pd.concat(all_data, ignore_index=True)\n",
    "    merged_data.rename(columns={original: cleaned for original, cleaned in zip(original_weather_param, cleaned_weather_param)}, inplace=True)\n",
    "    merged_data.to_csv(merged_csv_path, index=False)\n",
    "    print(f\"Merged CSV file saved at: {merged_csv_path}\")\n",
    "\n",
    "csv_dir = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Station_Data_csv\"\n",
    "months = [1]\n",
    "# shp_polygon = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\BBox_small.shp\"\n",
    "shp_polygon = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\BBox_large.shp\"\n",
    "weather_param = ['Temp (Â°C)', 'Wind Dir (10s deg)']\n",
    "\n",
    "filter_weather_data(csv_dir, months, shp_polygon, weather_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the wind directions in two class - 210-280 and all the remainings and output in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data has been saved to D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\merged_weather_data_BBox_large_1_yearly_pivoted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def process_weather_data(input_csv, aggregation='yearly'):\n",
    "\n",
    "    \"\"\"\n",
    "    Processes weather station data from a CSV file, counts wind directions within specific groups, and outputs a pivoted CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_csv: Path to the input CSV file containing weather station data.\n",
    "    - aggregation: Aggregation level, either 'yearly' or '5-yearly'.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(input_csv, low_memory=False).dropna(subset=['winddirsde'])\n",
    "    df['station name'] = df['station name'].astype(str).str.strip()\n",
    "    df['climate id'] = df['climate id'].astype(str).str.strip()\n",
    "    df['actual_wind_dir'] = df['winddirsde'] * 10\n",
    "\n",
    "    df['wind_dir_210_280'] = df['actual_wind_dir'].between(210, 280).astype(int)\n",
    "    df['wind_dir_281_209'] = (~df['actual_wind_dir'].between(210, 280)).astype(int)\n",
    "\n",
    "    if aggregation == '5-yearly':\n",
    "        df['aggregation_period'] = (df['year'] // 5 * 5).astype(str) + '-' + (df['year'] // 5 * 5 + 4).astype(str)\n",
    "        pivot_table = df.pivot_table(\n",
    "            values=['wind_dir_210_280', 'wind_dir_281_209'],\n",
    "            index=['station name', 'climate id', 'aggregation_period', 'month'],\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "    elif aggregation == 'yearly':\n",
    "        df['aggregation_period'] = df['year'].astype(str)\n",
    "        pivot_table = df.pivot_table(\n",
    "            values=['wind_dir_210_280', 'wind_dir_281_209'],\n",
    "            index=['station name', 'climate id', 'aggregation_period', 'month'],\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid aggregation level. Please specify either 'yearly' or '5-yearly'.\")\n",
    "\n",
    "    output_csv = Path(input_csv).with_name(f\"{Path(input_csv).stem}_{aggregation}_pivoted.csv\")\n",
    "    pivot_table.to_csv(output_csv, index=False)\n",
    "    print(f\"Processed data has been saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\merged_weather_data_BBox_large_1.csv\"\n",
    "process_weather_data(input_csv_path, aggregation='yearly')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
