{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download ECCC weather station data from https://dd.weather.gc.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "def download_csv_files(url, folder_path):\n",
    "    # Local directory where files will be saved using pathlib\n",
    "    save_dir = Path(folder_path)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    # Get the webpage content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to connect to {url}\")\n",
    "        return\n",
    "\n",
    "    # Parse the webpage content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the CSV file links\n",
    "    for link in soup.find_all('a'):\n",
    "        file_name = link.get('href')\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_url = url + file_name\n",
    "\n",
    "            # Download the CSV file\n",
    "            print(f\"Downloading {file_name}...\")\n",
    "            csv_response = requests.get(file_url)\n",
    "\n",
    "            # Save the CSV file using pathlib\n",
    "            file_path = save_dir / file_name\n",
    "            file_path.write_bytes(csv_response.content)\n",
    "\n",
    "    print(\"All files downloaded!\")\n",
    "\n",
    "# Example usage\n",
    "url = \"https://dd.weather.gc.ca/climate/observations/hourly/csv/AB/\"\n",
    "download_folder = r\"C:\\Users\\Sunbeam\\Downloads\\csv_files\"\n",
    "download_csv_files(url, download_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter and analyze station data and create a merged csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunbeam\\AppData\\Local\\Temp\\ipykernel_10368\\1628533076.py:43: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  weather_data['date/time (lst)'] = pd.to_datetime(weather_data['date/time (lst)'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORONATION A (ID: 3011880) - 1953 (8760), 1954 (8760), 1955 (8760), 1956 (8784), 1957 (8760), 1958 (8760), 1959 (8760), 1960 (8784), 1961 (8760), 1962 (8760), 1963 (8760), 1964 (8784), 1965 (8760), 1966 (8760), 1967 (8760), 1968 (8784), 1969 (8760), 1970 (8760), 1971 (8760), 1972 (8784), 1973 (8760), 1974 (8760), 1975 (8760), 1976 (8784), 1977 (8760), 1978 (8755), 1979 (8755), 1980 (8779), 1981 (8755), 1982 (8755), 1983 (8760), 1984 (8784), 1985 (8760), 1986 (8760), 1987 (8760), 1988 (8784), 1989 (8760), 1990 (8760), 1991 (8760), 1992 (8784), 1993 (8760), 1994 (2328)\n",
      "CORONATION (AUT) (ID: 3011885) - 1994 (8016), 1995 (8760), 1996 (8784), 1997 (8760), 1998 (8760), 1999 (8760), 2000 (8784), 2001 (8760), 2002 (8760), 2003 (8400)\n",
      "CORONATION CLIMATE (ID: 3011887) - 1999 (16), 2000 (30), 2001 (1508), 2002 (20), 2003 (1209), 2004 (8784), 2005 (8760), 2006 (8760), 2007 (8760), 2008 (8784), 2009 (8760), 2010 (8760), 2011 (8760), 2012 (8784), 2013 (8760), 2014 (8760), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (8784), 2021 (8760), 2022 (8760), 2023 (8760), 2024 (6960)\n",
      "DRUMHELLER EAST (ID: 30221LG) - 1994 (8016), 1995 (8760), 1996 (8769), 1997 (8760), 1998 (8760), 1999 (8760), 2000 (8784), 2001 (8759), 2002 (8760), 2003 (8760), 2004 (8784), 2005 (8760), 2006 (8760), 2007 (8760), 2008 (8784), 2009 (8760), 2010 (8760), 2011 (8760), 2012 (8784), 2013 (8760), 2014 (8760), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (8784), 2021 (8760), 2022 (8760), 2023 (8760), 2024 (6960)\n",
      "RED DEER A (ID: 3025480) - 1953 (8760), 1954 (8760), 1955 (8760), 1956 (8784), 1957 (8760), 1958 (8760), 1959 (8760), 1960 (8784), 1961 (8760), 1962 (8760), 1963 (8760), 1964 (8784), 1965 (8760), 1966 (8760), 1967 (8760), 1968 (8784), 1969 (8760), 1970 (8760), 1971 (8760), 1972 (8784), 1973 (8760), 1974 (8760), 1975 (8760), 1976 (8784), 1977 (8760), 1978 (8760), 1979 (8760), 1980 (8784), 1981 (8760), 1982 (8760), 1983 (8760), 1984 (8784), 1985 (8760), 1986 (8760), 1987 (8760), 1988 (8784), 1989 (8760), 1990 (8760), 1991 (8760), 1992 (8784), 1993 (8760), 1994 (8760), 1995 (8760), 1996 (8784), 1997 (8760), 1998 (8760), 1999 (8760), 2000 (8784), 2001 (8760), 2002 (8760), 2003 (8760), 2004 (8784), 2005 (8760), 2006 (8760), 2007 (8760), 2008 (8784), 2009 (8760), 2010 (8760), 2011 (8760), 2012 (8784), 2013 (8760), 2014 (2904)\n",
      "RED DEER REGIONAL A (ID: 3025481) - 2014 (5918), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (13860), 2021 (16824), 2022 (8760), 2023 (8760), 2024 (6960)\n",
      "RED DEER REGIONAL A (ID: 3025484) - 2014 (5918), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (13860), 2021 (16824), 2022 (8760), 2023 (8760), 2024 (6960)\n",
      "SUNDRE A (ID: 3026KNQ) - 1994 (8016), 1995 (8760), 1996 (8769), 1997 (8760), 1998 (8760), 1999 (8760), 2000 (8784), 2001 (8760), 2002 (8760), 2003 (8760), 2004 (8784), 2005 (8760), 2006 (8760), 2007 (8760), 2008 (8784), 2009 (8760), 2010 (8760), 2011 (8760), 2012 (8784), 2013 (8760), 2014 (8760), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (8784), 2021 (8760), 2022 (8760), 2023 (8760), 2024 (6960)\n",
      "CALGARY INTL A (ID: 3031092) - 2012 (4213), 2013 (8760), 2014 (8760), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (8784), 2021 (8760), 2022 (8760), 2023 (8760), 2024 (6960)\n",
      "CALGARY INT'L A (ID: 3031093) - 1953 (8760), 1954 (8760), 1955 (8760), 1956 (8784), 1957 (8760), 1958 (8760), 1959 (8760), 1960 (8784), 1961 (8760), 1962 (8760), 1963 (8760), 1964 (8784), 1965 (8760), 1966 (8760), 1967 (8760), 1968 (8784), 1969 (8760), 1970 (8760), 1971 (8760), 1972 (8784), 1973 (8760), 1974 (8760), 1975 (8760), 1976 (8784), 1977 (8760), 1978 (8760), 1979 (8760), 1980 (8784), 1981 (8760), 1982 (8760), 1983 (8760), 1984 (8784), 1985 (8760), 1986 (8760), 1987 (8760), 1988 (8784), 1989 (8760), 1990 (8760), 1991 (8760), 1992 (8784), 1993 (8760), 1994 (8760), 1995 (8760), 1996 (8784), 1997 (8760), 1998 (8760), 1999 (8760), 2000 (8784), 2001 (8760), 2002 (8760), 2003 (8760), 2004 (8784), 2005 (8760), 2006 (8760), 2007 (8760), 2008 (8784), 2009 (8760), 2010 (8760), 2011 (8760), 2012 (4656)\n",
      "CALGARY INT'L CS (ID: 3031094) - 2008 (226), 2009 (8760), 2010 (8760), 2011 (8760), 2012 (8784), 2013 (8760), 2014 (8760), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (8784), 2021 (8760), 2022 (8760), 2023 (8760), 2024 (6960)\n",
      "CLARESHOLM (ID: 3031640) - 1994 (8016), 1995 (8760), 1996 (8769), 1997 (8760), 1998 (8760), 1999 (8760), 2000 (8784), 2001 (8760), 2002 (8760), 2003 (8760), 2004 (8784), 2005 (8760), 2006 (8760), 2007 (8760), 2008 (8784), 2009 (8760), 2010 (8760), 2011 (8760), 2012 (8784), 2013 (8760), 2014 (8760), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (8784), 2021 (8760), 2022 (8760), 2023 (8760), 2024 (6960)\n",
      "LETHBRIDGE (ID: 3033875) - 2011 (8510), 2012 (8784), 2013 (8760), 2014 (8760), 2015 (8758), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (8784), 2021 (8760), 2022 (8760), 2023 (8760), 2024 (6960)\n",
      "LETHBRIDGE A (ID: 3033880) - 1953 (8760), 1954 (8760), 1955 (8760), 1956 (8784), 1957 (8760), 1958 (8760), 1959 (8760), 1960 (8784), 1961 (8760), 1962 (8760), 1963 (8760), 1964 (8784), 1965 (8760), 1966 (8760), 1967 (8760), 1968 (8784), 1969 (8760), 1970 (8760), 1971 (8760), 1972 (8784), 1973 (8760), 1974 (8760), 1975 (8760), 1976 (8784), 1977 (8760), 1978 (8760), 1979 (8760), 1980 (8784), 1981 (8760), 1982 (8760), 1983 (8760), 1984 (8784), 1985 (8760), 1986 (8760), 1987 (8760), 1988 (8784), 1989 (8760), 1990 (8760), 1991 (8760), 1992 (8784), 1993 (8760), 1994 (8760), 1995 (8760), 1996 (8784), 1997 (8760), 1998 (8760), 1999 (8760), 2000 (8784), 2001 (8760), 2002 (8760), 2003 (8760), 2004 (8784), 2005 (8760), 2006 (8760), 2007 (8754), 2008 (8778), 2009 (8754), 2010 (8754), 2011 (8754), 2012 (8863), 2013 (8754), 2014 (8754), 2015 (8754), 2016 (8778), 2017 (8754), 2018 (8754), 2019 (8754), 2020 (8778), 2021 (8754), 2022 (8754), 2023 (8754), 2024 (6930)\n",
      "LETHBRIDGE A (ID: 3033881) - 1953 (8760), 1954 (8760), 1955 (8760), 1956 (8784), 1957 (8760), 1958 (8760), 1959 (8760), 1960 (8784), 1961 (8760), 1962 (8760), 1963 (8760), 1964 (8784), 1965 (8760), 1966 (8760), 1967 (8760), 1968 (8784), 1969 (8760), 1970 (8760), 1971 (8760), 1972 (8784), 1973 (8760), 1974 (8760), 1975 (8760), 1976 (8784), 1977 (8760), 1978 (8760), 1979 (8760), 1980 (8784), 1981 (8760), 1982 (8760), 1983 (8760), 1984 (8784), 1985 (8760), 1986 (8760), 1987 (8760), 1988 (8784), 1989 (8760), 1990 (8760), 1991 (8760), 1992 (8784), 1993 (8760), 1994 (8760), 1995 (8760), 1996 (8784), 1997 (8760), 1998 (8760), 1999 (8760), 2000 (8784), 2001 (8760), 2002 (8760), 2003 (8760), 2004 (8784), 2005 (8760), 2006 (8760), 2007 (8754), 2008 (8778), 2009 (8754), 2010 (8754), 2011 (8754), 2012 (8863), 2013 (8754), 2014 (8754), 2015 (8754), 2016 (8778), 2017 (8754), 2018 (8754), 2019 (8754), 2020 (8778), 2021 (8754), 2022 (8754), 2023 (8754), 2024 (6930)\n",
      "LETHBRIDGE AWOS A (ID: 3033884) - 2006 (6697), 2007 (8760), 2008 (8784), 2009 (8760), 2010 (8760), 2011 (8760), 2012 (3912)\n",
      "LETHBRIDGE CDA (ID: 3033890) - 1994 (4982), 1995 (8757), 1996 (8768), 1997 (8760), 1998 (8760), 1999 (8760), 2000 (8784), 2001 (8760), 2002 (8760), 2003 (8760), 2004 (8784), 2005 (8760), 2006 (8760), 2007 (8760), 2008 (8783), 2009 (8760), 2010 (8759), 2011 (8759), 2012 (8783), 2013 (8759), 2014 (8760), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (8784), 2021 (8759), 2022 (8759), 2023 (8759), 2024 (6959)\n",
      "STRATHMORE AGDM (ID: 3036205) - 2004 (4726), 2005 (8760), 2006 (8760), 2007 (8760), 2008 (8784), 2009 (8760), 2010 (8760), 2011 (8688), 2012 (8723), 2013 (8760), 2014 (8760), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (8784), 2021 (8760), 2022 (8760), 2023 (8760), 2024 (6952)\n",
      "MILK RIVER (ID: 3044533) - 1994 (8016), 1995 (8760), 1996 (8769), 1997 (8760), 1998 (8760), 1999 (8760), 2000 (8784), 2001 (8760), 2002 (8760), 2003 (8760), 2004 (8784), 2005 (8760), 2006 (8760), 2007 (8760), 2008 (8784), 2009 (8760), 2010 (8760), 2011 (8760), 2012 (8784), 2013 (8760), 2014 (8760), 2015 (8760), 2016 (8784), 2017 (8760), 2018 (8760), 2019 (8760), 2020 (8784), 2021 (8760), 2022 (8760), 2023 (8760), 2024 (6960)\n",
      "Merged CSV file saved at: D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\ECCC_station_data_BBox_large_1_2_3_4_5_6_7_8_9_10_11_12.csv\n",
      "Station summary saved at: D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\ECCC_station_summary_BBox_large_1_2_3_4_5_6_7_8_9_10_11_12.csv\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "def filter_eccc_station_data(csv_dir, months, shp_bbox, weather_param=None):\n",
    "    \"\"\"\n",
    "    Filters weather data from CSV files in a directory based on specified months and spatial polygon, \n",
    "    merges the data, saves it as a combined CSV, and generates a summary per station.\n",
    "\n",
    "    Parameters:\n",
    "    csv_dir (str): Directory containing CSV files with weather data.\n",
    "    months (list): List of months (as integers) to filter the data.\n",
    "    shp_polygon (str): Path to the shapefile containing the polygon for spatial filtering.\n",
    "    weather_param (list, optional): List of weather parameters to include in the filtered data.\n",
    "    \"\"\"\n",
    "    \n",
    "    default_columns = ['longitude (x)', 'latitude (y)', 'station name', 'climate id', 'date/time (lst)', 'year', 'month', 'day', 'time (lst)']\n",
    "    weather_param = weather_param or []\n",
    "    original_weather_param = [param.lower() for param in weather_param]\n",
    "    cleaned_weather_param = [re.sub(r'[^a-zA-Z]', '', param.lower())[:10] for param in weather_param]\n",
    "    selected_columns = default_columns + original_weather_param\n",
    "\n",
    "    with arcpy.da.SearchCursor(shp_bbox, [\"SHAPE@\"]) as cursor:\n",
    "        bbox_polygon = next(cursor)[0]\n",
    "\n",
    "    csv_files = list(Path(csv_dir).glob(\"*.csv\"))\n",
    "    station_dict, lat_long_id_dict, all_data = {}, {}, []\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            weather_data = pd.read_csv(csv_file, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "        except UnicodeDecodeError:\n",
    "            weather_data = pd.read_csv(csv_file, encoding='ISO-8859-1', on_bad_lines='skip', engine='python')\n",
    "\n",
    "        weather_data.columns = weather_data.columns.str.strip().str.lower()\n",
    "        if 'station name' not in weather_data.columns:\n",
    "            continue\n",
    "\n",
    "        if 'date/time (lst)' in weather_data.columns:\n",
    "            weather_data['date/time (lst)'] = pd.to_datetime(weather_data['date/time (lst)'], errors='coerce')\n",
    "            weather_data = weather_data[weather_data['date/time (lst)'].dt.month.isin(months)]\n",
    "        if weather_data.empty:\n",
    "            continue\n",
    "\n",
    "        longitude_col = [col for col in weather_data.columns if 'longitude' in col][0]\n",
    "        latitude_col = [col for col in weather_data.columns if 'latitude' in col][0]\n",
    "        weather_data['station_point'] = weather_data.apply(\n",
    "            lambda row: arcpy.PointGeometry(arcpy.Point(row[longitude_col], row[latitude_col]), arcpy.SpatialReference(4326)), axis=1\n",
    "        )\n",
    "        weather_data = weather_data[weather_data['station_point'].apply(lambda pt: pt.within(bbox_polygon))]\n",
    "\n",
    "        if weather_data.empty:\n",
    "            continue\n",
    "\n",
    "        for _, row in weather_data.iterrows():\n",
    "            lat_long_id_key = (row[longitude_col], row[latitude_col], row['climate id'])\n",
    "            if lat_long_id_key not in lat_long_id_dict:\n",
    "                lat_long_id_dict[lat_long_id_key] = {\n",
    "                    'station_name': row['station name'],\n",
    "                    'climate_id': row['climate id'],\n",
    "                    'location': row['station_point']\n",
    "                }\n",
    "                station_dict.setdefault((row['station name'], row['climate id']), {'location': row['station_point'], 'data': pd.DataFrame()})\n",
    "\n",
    "        all_data.append(weather_data[selected_columns])\n",
    "\n",
    "    for (station_name, climate_id), station_info in station_dict.items():\n",
    "        station_data = pd.concat([data for data in all_data if data['station name'].eq(station_name).any()])\n",
    "        station_data['year'] = station_data['date/time (lst)'].dt.year\n",
    "        year_counts = station_data.groupby('year').size()\n",
    "        year_info = ', '.join([f\"{year} ({count})\" for year, count in year_counts.items()])\n",
    "        print(f\"{station_name} (ID: {climate_id}) - {year_info}\")\n",
    "    \n",
    "    if lat_long_id_dict:\n",
    "        shapefile_name = Path(shp_bbox).parent / f\"filtered_stations_{Path(shp_bbox).stem}_{'_'.join(map(str, months))}.shp\"\n",
    "        point_features = [info['location'] for info in lat_long_id_dict.values()]\n",
    "        arcpy.CopyFeatures_management(point_features, str(shapefile_name))\n",
    "        arcpy.management.AddFields(str(shapefile_name), [[\"station_n\", \"TEXT\"], [\"climate_id\", \"TEXT\"]] + [[field, \"TEXT\"] for field in cleaned_weather_param])\n",
    "\n",
    "        with arcpy.da.UpdateCursor(str(shapefile_name), [\"station_n\", \"climate_id\"] + cleaned_weather_param) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                key = list(lat_long_id_dict.keys())[i]\n",
    "                row[0], row[1] = lat_long_id_dict[key]['station_name'], lat_long_id_dict[key]['climate_id']\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    # Merge all the data and save as a CSV\n",
    "    merged_csv_path = Path(shp_bbox).parent / f\"ECCC_station_data_{Path(shp_bbox).stem}_{'_'.join(map(str, months))}.csv\"\n",
    "    merged_data = pd.concat(all_data, ignore_index=True)\n",
    "    merged_data.rename(columns={original: cleaned for original, cleaned in zip(original_weather_param, cleaned_weather_param)}, inplace=True)\n",
    "    merged_data.to_csv(merged_csv_path, index=False)\n",
    "    print(f\"Merged CSV file saved at: {merged_csv_path}\")\n",
    "\n",
    "    summary = []\n",
    "    for station_name, station_data in merged_data.groupby('station name'):\n",
    "        null_counts = station_data[cleaned_weather_param].isnull().sum().to_dict()\n",
    "\n",
    "        summary.append({\n",
    "            'Station name': station_name,\n",
    "            'First Year': station_data['date/time (lst)'].dt.year.min(),\n",
    "            'Total Records': len(station_data),\n",
    "            **{f\"Null Values - {param}\": null_counts.get(param, 0) for param in cleaned_weather_param}\n",
    "        })\n",
    "\n",
    "    # Save the summary as a CSV\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    summary_output_path = Path(shp_bbox).parent / f\"ECCC_station_summary_{Path(shp_bbox).stem}_{'_'.join(map(str, months))}.csv\"\n",
    "    summary_df.to_csv(summary_output_path, index=False)\n",
    "    print(f\"Station summary saved at: {summary_output_path}\")\n",
    "    \n",
    "# Example usage\n",
    "csv_dir = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ECCC_Data\"\n",
    "months = [i for i in range(1, 13)]\n",
    "# shp_polygon = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\BBox_small.shp\"\n",
    "shp_bbox = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\BBox_large.shp\"\n",
    "weather_param = ['Temp (°C)', 'Wind Dir (10s deg)']\n",
    "\n",
    "filter_eccc_station_data(csv_dir, months, shp_bbox, weather_param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the wind directions in two class - 210-280 and all the remainings and output in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in wind_dir_210_279 group: ['210-225', '225-240', '240-255', '255-270']\n",
      "Processed data has been saved to D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\Windir_1950_2024_Dec\\wind_direction_15deg_data_stream-oper_BBox_small_masked_yearly_pivoted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def process_eccc_station_data(input_csv, aggregation='yearly'):\n",
    "    \"\"\"\n",
    "    Processes weather station data from a CSV file, handling both individual measurements and binned counts.\n",
    "    Counts wind directions within specific groups (210-279 and 280-209) and outputs a pivoted CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv: Path to the input CSV file containing weather station data.\n",
    "    - aggregation: Aggregation level, either 'yearly' or '5-yearly'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv, low_memory=False)\n",
    "\n",
    "    # Detect if CSV format is binned (wind direction bins) or individual measurements\n",
    "    is_binned_format = any(re.match(r'^\\d+-\\d+$', col) for col in df.columns)\n",
    "\n",
    "    if is_binned_format:        \n",
    "        dir_210_279_bins = [col for col in df.columns if re.match(r'^(21[0-9]|2[0-7][0-9])-2[0-7][0-9]$', col)]\n",
    "        dir_other_bins = [col for col in df.columns if re.match(r'^\\d+-\\d+$', col) and col not in dir_210_279_bins]\n",
    "\n",
    "        print(\"Columns in wind_dir_210_279 group:\", dir_210_279_bins)\n",
    "\n",
    "        df['wind_dir_210_279'] = df[dir_210_279_bins].sum(axis=1)\n",
    "        df['wind_dir_280_209'] = df[dir_other_bins].sum(axis=1)\n",
    "    else:\n",
    "        df = df.dropna(subset=['winddirsde'])\n",
    "        df['actual_wind_dir'] = df['winddirsde'] * 10\n",
    "        df['wind_dir_210_279'] = df['actual_wind_dir'].between(210, 279).astype(int)\n",
    "        df['wind_dir_280_209'] = (~df['actual_wind_dir'].between(210, 279)).astype(int)\n",
    "\n",
    "    if aggregation == '5-yearly':\n",
    "        df['aggregation_period'] = (df['year'] // 5 * 5).astype(str) + '-' + (df['year'] // 5 * 5 + 4).astype(str)\n",
    "        pivot_table = df.pivot_table(\n",
    "            values=['wind_dir_210_279', 'wind_dir_280_209'],\n",
    "            index=['aggregation_period', 'month'],\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "    elif aggregation == 'yearly':\n",
    "        df['aggregation_period'] = df['year'].astype(str)\n",
    "        pivot_table = df.pivot_table(\n",
    "            values=['wind_dir_210_279', 'wind_dir_280_209'],\n",
    "            index=['aggregation_period', 'month'],\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid aggregation level. Please specify either 'yearly' or '5-yearly'.\")\n",
    "\n",
    "    # Save the results to a new CSV file\n",
    "    output_csv = Path(input_csv).with_name(f\"{Path(input_csv).stem}_{aggregation}_pivoted.csv\")\n",
    "    pivot_table.to_csv(output_csv, index=False)\n",
    "    print(f\"Processed data has been saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\Windir_1950_2024_Dec\\wind_direction_15deg_data_stream-oper_BBox_small_masked.csv\"\n",
    "# input_csv_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\Windir_1950_2024_Dec\\wind_direction_15deg_data_stream-oper_BBox_large_masked.csv\"\n",
    "process_eccc_station_data(input_csv_path, aggregation='yearly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process ACIS weather station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to 'D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ACIS_Data\\ACIS_station_data.csv'.\n",
      "Station summary saved to 'D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ACIS_Data\\ACIS_station_summary.csv'.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def process_acis_data(acis_data_path, coords_data_path):\n",
    "    acis_data_path = Path(acis_data_path)\n",
    "    output_dir = acis_data_path.parent\n",
    "    \n",
    "    # Load data\n",
    "    acis_data = pd.read_csv(acis_data_path, low_memory=False)\n",
    "    coords_data = pd.read_excel(coords_data_path)\n",
    "    \n",
    "    # Process data\n",
    "    wd_at_data = acis_data[acis_data['SENSOR_CD'].isin(['WD', 'AT'])].copy()\n",
    "    wd_at_data['TIMESTAMP'] = pd.to_datetime(wd_at_data['TIMESTAMP'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
    "    wd_at_data['READING'] = pd.to_numeric(wd_at_data['READING'], errors='coerce')\n",
    "    \n",
    "    wd_at_pivot = wd_at_data.pivot_table(index=['NAME', 'TIMESTAMP'], \n",
    "                                         columns='SENSOR_CD', \n",
    "                                         values='READING', \n",
    "                                         aggfunc='first').reset_index()\n",
    "    wd_at_pivot.columns.name = None\n",
    "    \n",
    "    merged_data = pd.merge(wd_at_pivot, coords_data, left_on='NAME', right_on='Station names', how='left')[\n",
    "        ['NAME', 'TIMESTAMP', 'WD', 'AT', 'X', 'Y']\n",
    "    ]\n",
    "    merged_data.columns = ['station name', 'datetime', 'winddir', 'atmostemp', 'longitude', 'latitude']\n",
    "    \n",
    "    # Generate merged CSV\n",
    "    output_path = output_dir / \"ACIS_station_data.csv\"\n",
    "    merged_data.to_csv(output_path, index=False)\n",
    "    print(f\"Merged data saved to '{output_path}'.\")\n",
    "\n",
    "    # Generate summary for each station\n",
    "    summary = []\n",
    "    for station, group in merged_data.groupby('station name'):\n",
    "        first_year = group['datetime'].dt.year.min()\n",
    "        total_records = len(group)\n",
    "        null_counts = group[['winddir', 'atmostemp']].isnull().sum().to_dict()  # Count nulls for each parameter\n",
    "        \n",
    "        summary.append({\n",
    "            'Station name': station,\n",
    "            'First Year': first_year,\n",
    "            'Total Records': total_records,\n",
    "            'Null Values - winddir': null_counts.get('winddir', 0),\n",
    "            'Null Values - atmostemp': null_counts.get('atmostemp', 0)\n",
    "        })\n",
    "    \n",
    "    # Convert summary to DataFrame\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    \n",
    "    # Generate summary CSV\n",
    "    summary_output_path = output_dir / \"ACIS_station_summary.csv\"\n",
    "    summary_df.to_csv(summary_output_path, index=False)\n",
    "    print(f\"Station summary saved to '{summary_output_path}'.\")\n",
    "\n",
    "# Example usage\n",
    "acis_data_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ACIS_Data\\select stn hrly data dec-feb at wd.csv\"\n",
    "coords_data_path =  r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ACIS_Data\\ACIS_Station_names.xlsx\"\n",
    "process_acis_data(acis_data_path, coords_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stations within the bounding box: 27\n",
      "Filtered ACIS data saved at: D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\filtered_acis_data_BBox_large_1.csv\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "def filter_acis_station_data(csv_path, months, shp_polygon, weather_param=None):\n",
    "    \"\"\"\n",
    "    Filters ACIS weather station data based on specified months and a spatial polygon,\n",
    "    and saves the filtered data as a CSV.\n",
    "\n",
    "    Parameters:\n",
    "    csv_path (str): Path to the ACIS merged CSV file.\n",
    "    months (list): List of months (as integers) to filter the data.\n",
    "    shp_polygon (str): Path to the shapefile containing the polygon for spatial filtering.\n",
    "    weather_param (list, optional): List of weather parameters to include in the filtered data. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    default_columns = ['longitude', 'latitude', 'station name', 'datetime']\n",
    "    weather_param = weather_param or ['winddir', 'atmostemp']\n",
    "    \n",
    "    # Load data\n",
    "    acis_data = pd.read_csv(csv_path, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "    acis_data.columns = acis_data.columns.str.strip().str.lower()\n",
    "\n",
    "    # Check that all columns are in the DataFrame\n",
    "    selected_columns = default_columns + [col for col in weather_param if col in acis_data.columns]\n",
    "\n",
    "    # Filter by specified months\n",
    "    acis_data['datetime'] = pd.to_datetime(acis_data['datetime'], errors='coerce')\n",
    "    acis_data = acis_data[acis_data['datetime'].dt.month.isin(months)]\n",
    "    if acis_data.empty:\n",
    "        print(\"No data available for the specified months.\")\n",
    "        return\n",
    "\n",
    "    # Spatial filtering\n",
    "    with arcpy.da.SearchCursor(shp_polygon, [\"SHAPE@\"]) as cursor:\n",
    "        bbox_polygon = next(cursor)[0]\n",
    "\n",
    "    acis_data['station_point'] = acis_data.apply(\n",
    "        lambda row: arcpy.PointGeometry(arcpy.Point(row['longitude'], row['latitude']), arcpy.SpatialReference(4326)), axis=1\n",
    "    )\n",
    "    acis_data = acis_data[acis_data['station_point'].apply(lambda pt: pt.within(bbox_polygon))]\n",
    "\n",
    "    if acis_data.empty:\n",
    "        print(\"No data found within the specified polygon.\")\n",
    "        return\n",
    "\n",
    "    # Count unique stations within the bounding box\n",
    "    unique_stations_count = acis_data['station name'].nunique()\n",
    "    print(f\"Number of stations within the bounding box: {unique_stations_count}\")\n",
    "\n",
    "    # Select only the columns that exist in the data\n",
    "    acis_data = acis_data[selected_columns]\n",
    "    \n",
    "    # Output filtered data as CSV\n",
    "    output_path = Path(shp_polygon).parent / f\"acis_filtered_acis_data_{Path(shp_polygon).stem}_{'_'.join(map(str, months))}.csv\"\n",
    "    acis_data.to_csv(output_path, index=False)\n",
    "    print(f\"Filtered ACIS data saved at: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "csv_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ACIS_Data\\merged_station_data.csv\"\n",
    "months = [1]\n",
    "shp_bbox = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\BBox_large.shp\"\n",
    "weather_param = ['winddir', 'atmostemp']\n",
    "\n",
    "filter_acis_station_data(csv_path, months, shp_bbox, weather_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind direction hour counts saved to D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\acis_wind_direction_15deg_filtered_acis_data_BBox_large_1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def calculate_wind_direction_hour_counts_to_csv(input_csv_file, bin_size=15, cutoff_day=None):\n",
    "    \"\"\"\n",
    "    Processes ACIS wind data from a CSV to calculate the number of hours the wind falls within specific direction bins \n",
    "    for each station and month, preserving hourly counts.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv_file: Path to the input CSV file.\n",
    "    - bin_size: Wind direction bin size in degrees (default is 15°).\n",
    "    - cutoff_day: Limits data to days 1 through cutoff_day per month (default is None for full month).\n",
    "    \"\"\"\n",
    "    # Load CSV data\n",
    "    df = pd.read_csv(input_csv_file)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    df.dropna(subset=['datetime', 'winddir', 'station name'], inplace=True)\n",
    "    \n",
    "    # Apply day cutoff if specified\n",
    "    if cutoff_day:\n",
    "        df = df[df['datetime'].dt.day <= cutoff_day]\n",
    "    \n",
    "    # Extract year, month, and ensure wind direction is in degrees\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    \n",
    "    # Bin wind direction data into specified intervals\n",
    "    bins = np.arange(0, 360 + bin_size, bin_size)\n",
    "    df['wind_dir_bin'] = pd.cut(df['winddir'], bins=bins, \n",
    "                                labels=[f\"{int(b)}-{int(b + bin_size)}\" for b in bins[:-1]], right=False)\n",
    "\n",
    "    # Count occurrences within each bin by station, year, and month\n",
    "    wind_direction_counts = df.groupby(['station name', 'year', 'month', 'wind_dir_bin']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Save output to CSV\n",
    "    output_csv_file = Path(input_csv_file).with_name(f\"acis_wind_direction_{bin_size}deg_{Path(input_csv_file).stem}.csv\")\n",
    "    wind_direction_counts.to_csv(output_csv_file)\n",
    "    print(f\"Wind direction hour counts saved to {output_csv_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_file = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\filtered_acis_data_BBox_large_1.csv\"\n",
    "calculate_wind_direction_hour_counts_to_csv(input_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data has been saved to D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\acis_wind_direction_15deg_filtered_acis_data_BBox_large_1_yearly_pivoted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def process_acis_binned_data(input_csv, aggregation='yearly'):\n",
    "    \"\"\"\n",
    "    Processes ACIS weather station data with pre-binned wind directions, counting hours within specific direction groups\n",
    "    (210-279 and 280-209) and outputs a pivoted CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv: Path to the input ACIS CSV file containing pre-binned wind direction data.\n",
    "    - aggregation: Aggregation level, either 'yearly' or '5-yearly'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv, low_memory=False)\n",
    "\n",
    "    # Identify columns corresponding to each wind direction bin\n",
    "    dir_210_279_bins = [col for col in df.columns if re.match(r'^(21[0-9]|2[0-7][0-9])-2[0-7][0-9]$', col)]\n",
    "    dir_other_bins = [col for col in df.columns if re.match(r'^\\d+-\\d+$', col) and col not in dir_210_279_bins]\n",
    "\n",
    "    # Sum counts for the two direction groups\n",
    "    df['wind_dir_210_279'] = df[dir_210_279_bins].sum(axis=1)\n",
    "    df['wind_dir_280_209'] = df[dir_other_bins].sum(axis=1)\n",
    "\n",
    "    # Determine aggregation period\n",
    "    if aggregation == '5-yearly':\n",
    "        df['aggregation_period'] = (df['year'] // 5 * 5).astype(str) + '-' + (df['year'] // 5 * 5 + 4).astype(str)\n",
    "        pivot_table = df.pivot_table(\n",
    "            values=['wind_dir_210_279', 'wind_dir_280_209'],\n",
    "            index=['station name', 'aggregation_period', 'month'],\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "    elif aggregation == 'yearly':\n",
    "        df['aggregation_period'] = df['year'].astype(str)\n",
    "        pivot_table = df.pivot_table(\n",
    "            values=['wind_dir_210_279', 'wind_dir_280_209'],\n",
    "            index=['station name', 'aggregation_period', 'month'],\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid aggregation level. Please specify either 'yearly' or '5-yearly'.\")\n",
    "\n",
    "    # Save the results to a new CSV file\n",
    "    output_csv = Path(input_csv).with_name(f\"{Path(input_csv).stem}_{aggregation}_pivoted.csv\")\n",
    "    pivot_table.to_csv(output_csv, index=False)\n",
    "    print(f\"Processed data has been saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\acis_wind_direction_15deg_filtered_acis_data_BBox_large_1.csv\"\n",
    "process_acis_binned_data(input_csv_path, aggregation='yearly')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
