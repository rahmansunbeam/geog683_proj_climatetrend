{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process missing weather station data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill missing ECCC station data with interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def eccc_fill_wind_dir_gaps(input_csv):\n",
    "    \"\"\"\n",
    "    Reads wind direction data, checks for gaps in 'date/time (lst)' within a 7-day threshold,\n",
    "    fills missing rows, and interpolates 'winddirsde' using nearest-neighbor interpolation \n",
    "    grouped by 'climate id' and 'month'.\n",
    "\n",
    "    Parameters:\n",
    "    input_csv (str or Path): Path to the input CSV file containing required columns.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(input_csv, low_memory=False, parse_dates=['date/time (lst)'])\n",
    "    required_columns = ['climate id', 'year', 'month', 'day', 'time (lst)', \n",
    "                        'longitude (x)', 'latitude (y)', 'station name', 'winddirsde']\n",
    "    if not all(col in data.columns for col in required_columns):\n",
    "        raise ValueError(f\"Input CSV must contain {required_columns}\")\n",
    "\n",
    "    # Sort and process groups to fill gaps in 'date/time (lst)' within 7 days\n",
    "    filled_data = []\n",
    "    for (climate_id, year, month), group in data.sort_values(by=['climate id', 'year', 'month', 'date/time (lst)']).groupby(['climate id', 'year', 'month']):\n",
    "        if (group['date/time (lst)'].diff().dt.total_seconds() / 3600).max() > 1:\n",
    "            group = group.set_index('date/time (lst)').resample('H').asfreq().reset_index()\n",
    "            group[['climate id', 'year', 'month']] = climate_id, year, month\n",
    "            group['day'], group['time (lst)'] = group['date/time (lst)'].dt.day, group['date/time (lst)'].dt.time\n",
    "            group[['longitude (x)', 'latitude (y)', 'station name']] = group[['longitude (x)', 'latitude (y)', 'station name']].ffill().bfill()\n",
    "        filled_data.append(group)\n",
    "\n",
    "    # Concatenate and interpolate 'winddirsde'\n",
    "    data_filled = pd.concat(filled_data).reset_index(drop=True)\n",
    "    data_filled['winddirsde'] = data_filled.groupby(['climate id', 'month'])['winddirsde'].transform(\n",
    "        lambda x: x.interpolate(method='nearest', limit_direction='both')\n",
    "    )\n",
    "\n",
    "    # Save output\n",
    "    output_file = Path(input_csv).with_name(f\"{Path(input_csv).stem}_filled.csv\")\n",
    "    data_filled.to_csv(output_file, index=False, float_format='%.2f')\n",
    "    print(f\"Filled data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "csv_file = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\ECCC_station_data_BBox_small_12_1_2.csv\"\n",
    "eccc_fill_wind_dir_gaps(csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill missing ACIS station data with interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def acis_fill_wind_dir_gaps(input_csv):\n",
    "    \"\"\"\n",
    "    Reads wind direction data, fills missing 'datetime' rows within a 7-day threshold and interpolates 'winddir'\n",
    "    grouped by 'station name' and 'year', then saves the filled data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    input_csv (str or Path): Path to the input CSV file containing 'station name', 'datetime', and 'winddir' columns.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(input_csv, low_memory=False, parse_dates=['datetime'])\n",
    "    if not all(col in data.columns for col in ['station name', 'datetime', 'winddir']):\n",
    "        raise ValueError(\"Input CSV must contain 'station name', 'datetime', and 'winddir' columns.\")\n",
    "\n",
    "    data['year'] = data['datetime'].dt.year\n",
    "    data = data.sort_values(['station name', 'year', 'datetime'])\n",
    "\n",
    "    # Process groups and fill missing rows in 'datetime' within a 7-day threshold\n",
    "    filled_data = []\n",
    "    for (station_name, year), group in data.groupby(['station name', 'year']):\n",
    "        if (group['datetime'].diff().dt.total_seconds() / 3600).max() > 1:\n",
    "            group = group.set_index('datetime').resample('H').asfreq().reset_index()\n",
    "            group[['station name', 'year']] = station_name, year\n",
    "        filled_data.append(group)\n",
    "\n",
    "    data_filled = pd.concat(filled_data).reset_index(drop=True)\n",
    "\n",
    "    # Interpolate 'winddir' for each station and year\n",
    "    data_filled['winddir'] = data_filled.groupby(['station name', 'year'])['winddir'].transform(\n",
    "        lambda x: x.interpolate(method='nearest', limit_direction='both')\n",
    "    )\n",
    "\n",
    "    # Save output\n",
    "    output_file = Path(input_csv).with_name(f\"{Path(input_csv).stem}_filled.csv\")\n",
    "    data_filled.drop(columns=['year']).to_csv(output_file, index=False)\n",
    "    print(f\"Filled data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "csv_file = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ACIS_Data\\ACIS_station_data.csv\"\n",
    "acis_fill_wind_dir_gaps(csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the ERA5 NC data with station data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate using ECCC filled CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "def validate_era5_station_data_by_eccc_station_data(nc_file, station_csv, station_names):\n",
    "    \"\"\"\n",
    "    Processes ERA5 NC data and station CSV data to extract daily mean wind direction\n",
    "    and match data by time, generating an output CSV with comparison.\n",
    "\n",
    "    Parameters:\n",
    "    - nc_file (str): Path to the ERA5 NetCDF file.\n",
    "    - station_csv (str): Path to the station CSV file.\n",
    "    - station_names (list): List of station names to process.\n",
    "    \"\"\"\n",
    "    station_data = pd.read_csv(station_csv, low_memory=False)\n",
    "    station_data['date/time (lst)'] = pd.to_datetime(station_data['date/time (lst)'], errors='coerce')\n",
    "    station_data = station_data[station_data['station name'].isin(station_names)]\n",
    "    station_daily = station_data.groupby(\n",
    "        ['station name', 'climate id', 'latitude (y)', 'longitude (x)', 'year', 'month', 'day']\n",
    "    ).agg({'winddirsde': lambda x: np.mean(x.dropna() * 10) if len(x.dropna()) > 0 else np.nan}).reset_index()\n",
    "    station_daily.rename(columns={'winddirsde': 'Y (mean direction of weath st)'}, inplace=True)\n",
    "\n",
    "    era5_data = xr.open_dataset(nc_file, engine='netcdf4')\n",
    "    nc_years_months = set(pd.to_datetime(era5_data['valid_time'].values).strftime('%Y-%m'))\n",
    "\n",
    "    def calculate_mean_wind_direction(u, v):\n",
    "        speed = np.sqrt(u ** 2 + v ** 2)\n",
    "        rad = (np.arctan2(u, v) + 2 * np.pi) % (2 * np.pi)\n",
    "        return np.rad2deg((np.arctan2(np.mean(np.sin(rad) * speed), np.mean(np.cos(rad) * speed)) + 2 * np.pi) % (2 * np.pi))\n",
    "\n",
    "    output = []\n",
    "    for _, variant in station_daily[['latitude (y)', 'longitude (x)', 'climate id']].drop_duplicates().iterrows():\n",
    "        u, v = era5_data['u'].sel(latitude=variant['latitude (y)'], longitude=variant['longitude (x)'], method='nearest'), \\\n",
    "               era5_data['v'].sel(latitude=variant['latitude (y)'], longitude=variant['longitude (x)'], method='nearest')\n",
    "        time_indices = pd.to_datetime(u['valid_time'].values)\n",
    "        daily = pd.DataFrame([\n",
    "            (day, calculate_mean_wind_direction(\n",
    "                u.values[time_indices.floor('D') == day],\n",
    "                v.values[time_indices.floor('D') == day]\n",
    "            ))\n",
    "            for day in time_indices.to_series().dt.floor('D').unique()\n",
    "        ], columns=['time', 'X (mean direction of NC)'])\n",
    "\n",
    "        daily['year'] = daily['time'].dt.year\n",
    "        daily['month'] = daily['time'].dt.month\n",
    "        daily['day'] = daily['time'].dt.day\n",
    "\n",
    "        filtered = station_daily[\n",
    "            (station_daily['climate id'] == variant['climate id']) &\n",
    "            (station_daily[['year', 'month']].apply(lambda x: f\"{x[0]}-{x[1]:02}\", axis=1).isin(nc_years_months))\n",
    "        ]\n",
    "        output.append(pd.merge(daily, filtered, on=['year', 'month', 'day'], how='inner'))\n",
    "\n",
    "    pd.concat(output, ignore_index=True)[[\n",
    "        'station name', 'climate id', 'latitude (y)', 'longitude (x)', 'year', 'month', 'day',\n",
    "        'X (mean direction of NC)', 'Y (mean direction of weath st)'\n",
    "    ]].to_csv(Path(nc_file).with_name(f\"{Path(nc_file).stem}_{'_'.join(station_names)}_ECCC_comparison.csv\"), index=False)\n",
    "\n",
    "    print(f\"Output saved to {Path(nc_file).with_name(f'{Path(nc_file).stem}_{'_'.join(station_names)}_ECCC_comparison.csv')}\")\n",
    "\n",
    "# Core\n",
    "# nc_file = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\Windir_1950_2024_Feb\\data_stream-oper_BBox_small_masked.nc\"\n",
    "# station_csv = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\ECCC_station_data_BBox_small_12_1_2_filled.csv\"\n",
    "# station_names = [\"BROOKS\", \"MEDICINE HAT A\", \"SUFFIELD A\"]\n",
    "\n",
    "# Outer core\n",
    "nc_file = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\Windir_1950_2024_Dec\\data_stream-oper_BBox_large_masked.nc\"\n",
    "station_csv = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\BBox\\ECCC_station_data_BBox_large_12_1_2_filled.csv\"\n",
    "station_names = [\"CALGARY INT'L A\", \"LETHBRIDGE A\"]\n",
    "\n",
    "validate_era5_station_data_by_eccc_station_data(nc_file, station_csv, station_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate using ACIS filled CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "def validate_era5_station_data_by_acis_station_data(nc_file, station_csv, station_names):\n",
    "    \"\"\"\n",
    "    Processes ERA5 NC data and ACIS station CSV data to extract daily mean wind direction\n",
    "    and match data by time, generating an output CSV with comparison.\n",
    "\n",
    "    Parameters:\n",
    "    - nc_file (str): Path to the ERA5 NetCDF file.\n",
    "    - station_csv (str): Path to the ACIS station CSV file.\n",
    "    - station_names (list): List of station names to process.\n",
    "    \"\"\"\n",
    "    station_data = pd.read_csv(station_csv, low_memory=False)\n",
    "    station_data['datetime'] = pd.to_datetime(station_data['datetime'], errors='coerce')\n",
    "    station_data = station_data[station_data['station name'].isin(station_names)]\n",
    "    \n",
    "    station_daily = station_data.groupby(\n",
    "        ['station name', 'latitude', 'longitude', station_data['datetime'].dt.year.rename('year'), station_data['datetime'].dt.month.rename('month'), station_data['datetime'].dt.day.rename('day')]\n",
    "    ).agg({'winddir': lambda x: np.mean(x.dropna()), 'atmostemp': lambda x: np.mean(x.dropna())}).reset_index()\n",
    "    station_daily.rename(columns={\n",
    "        'winddir': 'Y (mean direction of station)',\n",
    "        'atmostemp': 'Y (mean temp of station)'\n",
    "    }, inplace=True)\n",
    "\n",
    "    era5_data = xr.open_dataset(nc_file, engine='netcdf4')\n",
    "    nc_time = pd.to_datetime(era5_data['valid_time'].values)\n",
    "    nc_years_months = set(nc_time.strftime('%Y-%m'))\n",
    "\n",
    "    def calculate_mean_wind_direction(u, v):\n",
    "        speed = np.sqrt(u ** 2 + v ** 2)\n",
    "        rad = (np.arctan2(u, v) + 2 * np.pi) % (2 * np.pi)\n",
    "        return np.rad2deg((np.arctan2(np.mean(np.sin(rad) * speed), np.mean(np.cos(rad) * speed)) + 2 * np.pi) % (2 * np.pi))\n",
    "\n",
    "    output = []\n",
    "    for _, variant in station_daily[['latitude', 'longitude']].drop_duplicates().iterrows():\n",
    "        u, v = era5_data['u'].sel(latitude=variant['latitude'], longitude=variant['longitude'], method='nearest'), \\\n",
    "               era5_data['v'].sel(latitude=variant['latitude'], longitude=variant['longitude'], method='nearest')\n",
    "        time_indices = pd.to_datetime(u['valid_time'].values)\n",
    "        daily = pd.DataFrame([\n",
    "            (day, calculate_mean_wind_direction(\n",
    "                u.values[time_indices.floor('D') == day],\n",
    "                v.values[time_indices.floor('D') == day]\n",
    "            ))\n",
    "            for day in time_indices.to_series().dt.floor('D').unique()\n",
    "        ], columns=['time', 'X (mean direction of NC)'])\n",
    "\n",
    "        daily['year'] = daily['time'].dt.year\n",
    "        daily['month'] = daily['time'].dt.month\n",
    "        daily['day'] = daily['time'].dt.day\n",
    "\n",
    "        filtered = station_daily[\n",
    "            (station_daily['latitude'] == variant['latitude']) &\n",
    "            (station_daily['longitude'] == variant['longitude']) &\n",
    "            (station_daily[['year', 'month']].apply(lambda x: f\"{x[0]}-{x[1]:02}\", axis=1).isin(nc_years_months))\n",
    "        ]\n",
    "        output.append(pd.merge(daily, filtered, on=['year', 'month', 'day'], how='inner'))\n",
    "\n",
    "    pd.concat(output, ignore_index=True)[[\n",
    "        'station name', 'latitude', 'longitude', 'year', 'month', 'day',\n",
    "        'X (mean direction of NC)', 'Y (mean direction of station)', 'Y (mean temp of station)'\n",
    "    ]].to_csv(Path(nc_file).with_name(f\"{Path(nc_file).stem}_{'_'.join(station_names)}_ACIS_comparison.csv\"), index=False)\n",
    "\n",
    "    print(f\"Output saved to {Path(nc_file).with_name(f'{Path(nc_file).stem}_{'_'.join(station_names)}_ACIS_comparison.csv')}\")\n",
    "\n",
    "# Example usage\n",
    "# nc_file = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\Windir_1950_2024_Dec\\data_stream-oper_BBox_small_masked.nc\"\n",
    "nc_file = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\Windir_1950_2024_Feb\\data_stream-oper_BBox_large_masked.nc\"\n",
    "station_csv = r\"D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\ACIS_Data\\ACIS_station_data_filled.csv\"\n",
    "station_names = [\"Hussar AGDM\", \"Acadia Valley AGCM\"]\n",
    "\n",
    "validate_era5_station_data_by_acis_station_data(nc_file, station_csv, station_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the number significant values per station names using Shapiro-Wilks test on residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station-Level Summary with P-Values, Year Counts, and Data Points:\n",
      "              Station  P-Value > 0.05  P-Value <= 0.05  Total Years  \\\n",
      "0  Acadia Valley AGCM               0               17           17   \n",
      "1              Brooks               0               20           20   \n",
      "2     Fincastle IMCIN               0               20           20   \n",
      "3         Hussar AGDM               0               20           20   \n",
      "4         Onefour CDA               0               20           20   \n",
      "\n",
      "   Total Data Points  \n",
      "0               1444  \n",
      "1               1713  \n",
      "2               1713  \n",
      "3               1713  \n",
      "4               1713  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def analyze_station_pvalues(file_path, sheet_name='Sheet1', residual_column='Residual (y-x)', \n",
    "                            year_column='year', station_column='station name'):\n",
    "    \"\"\"\n",
    "    Analyzes the normality of residuals for each station and year using the Shapiro-Wilk test.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the Excel file containing the data.\n",
    "    sheet_name (str): Sheet name in the Excel file to load. Default is 'Sheet1'.\n",
    "    residual_column (str): Name of the column containing residual values. Default is 'Residual (y-x)'.\n",
    "    year_column (str): Name of the column containing year values. Default is 'year'.\n",
    "    station_column (str): Name of the column containing station names. Default is 'station name'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame summarizing the results for each station.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the data\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "        # Group by station and analyze\n",
    "        station_results = []\n",
    "        for station, station_data in data.groupby(station_column):\n",
    "            p_value_above_05 = 0\n",
    "            total_years = station_data[year_column].nunique()\n",
    "            total_data_points = station_data.shape[0]\n",
    "\n",
    "            for year, year_data in station_data.groupby(year_column):\n",
    "                if len(year_data[residual_column]) >= 3:  # Minimum data points for Shapiro test\n",
    "                    _, p_value = shapiro(year_data[residual_column])\n",
    "                    p_value_above_05 += (p_value > 0.05)\n",
    "\n",
    "            # Append results for the station\n",
    "            station_results.append({\n",
    "                'Station': station,\n",
    "                'P-Value > 0.05': p_value_above_05,\n",
    "                'P-Value <= 0.05': total_years - p_value_above_05,\n",
    "                'Total Years': total_years,\n",
    "                'Total Data Points': total_data_points\n",
    "            })\n",
    "\n",
    "        # Convert results to a DataFrame\n",
    "        results_df = pd.DataFrame(station_results)\n",
    "        return results_df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = r'D:\\UCalgary_Lectures\\GEOG_683\\Data_workspace\\Daily_multilevel\\data_stream-oper_BBox_small_masked_ACIS_DJF_comparison.xlsx'\n",
    "analyze_station_pvalues(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
